# 为什么 Negative Sample Reinforcement 只罚错就能提升推理？

**陈丹琦团队新作 NeurIPS 2025**
The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning
https://www.arxiv.org/pdf/2506.01347

## 📌 面试核心回答框架

### 💡 一句话回答

> **核心结论：** RLVR 可以拆成 **正样本强化（PSR）** 和 **负样本强化（NSR）**，而 **单独使用 NSR 就能激活模型潜在推理能力**，在数学推理任务上与甚至超过 PPO/GRPO，同时保留多样性；原因在于 **惩罚错误路径比奖励正确答案更能保护先验知识并维持探索**。

---

## 📝 详细回答（3-5分钟）

### 1️⃣ 核心问题（30秒）

**传统RL（PPO/GRPO）的问题：**

| 维度 | 现象 | 问题 |
|------|------|------|
| **奖励信号混合** | 正负样本同池优化 | 梯度相互抵消，信号稀释 |
| **多样性-准确率折中** | Pass@1↑伴随多样性↓ | 出现模式崩溃（EE-Tradeoff） |
| **先验知识损失** | 长期训练后性能下滑 | 错误惩罚不足导致“坏习惯”固化 |

**论文反直觉发现：**
```
只用负样本：告诉模型“答案错了”的频率 > 告诉模型“答案对了”的价值
```

---

### 2️⃣ Negative RL 核心原理（2分钟）

#### ✅ RLVR 拆解：PSR vs NSR

| 模块 | 定义 | 梯度方向 | 直观效果 |
|------|------|----------|----------|
| **PSR**（Positive Sample Reinforcement） | 仅奖励正确答案 | 放大当前正确token logit，压制其余token | 精确收敛，但易丢多样性 |
| **NSR**（Negative Sample Reinforcement） | 仅惩罚错误答案 | 降低错误token logit，将概率按当前分布重分配 | 保持探索，压制错误路径 |

> **洞察：** 当模型基础知识不错时，错误样本中往往包含“错误的推理分支”。**NSR 直接针对这些分支**，而 **PSR 可能把“确定性”拉满导致熵崩溃**。

#### ✅ 梯度视角的机制解释

- **PSR 梯度（奖励 +1）：** 提高生成正确token的概率，同时强压所有其它token → 多样性下降。
- **NSR 梯度（惩罚 -1）：** 降低错误token概率，但按照当前概率比例把权重重新分给其它候选 → 既能舍弃错误，又能保留备选路径。
- **自适应保护：** 当错误答案概率接近 1（模型非常自信时），NSR 梯度因 $(1-\pi)$ 因子而自然衰减 → 不会破坏语法、常识等基础知识。

---

### 3️⃣ 关键实验发现（1.5分钟）

论文以 **Qwen2.5-Math-7B、Qwen3-4B、Llama-3.1-8B** 在 **MATH / AIME2025 / AMC23** 等数据集上验证：

| 结论 | 证据 | 意义 |
|------|------|------|
| **NSR-only ≈ 完整RL** | Pass@1 接近甚至超过 PPO/GRPO | 不看正样本也能学会“少犯错” |
| **多样性保持** | Pass@k（k≥32）持续提升，曲线优于 PSR | 采样质量与数量兼得 |
| **潜力激活** | Qwen3-4B 非思考模式实验中，NSR 将性能推到接近“考试模式” | NSR 更像“去噪”而非“填鸭” |

**对比：** PSR 虽在 Pass@1 初期有优势，但 k>8 后低于基线，呈现模式崩溃；NSR 在所有 k 值持续上升。

---

### 4️⃣ 为什么“只罚错”能保护先验？

**关键洞察：** NSR 的目标是 **错误抑制 + 先验引导的概率重分配**：

1. **错误路径被针对**：惩罚发生在导致错误的具体 token 序列上。
2. **梯度自动衰减**：错误 token 的概率越高，$(1-\pi)$ 越小 → 避免把语法/常识拖垮。
3. **知识被重新分配**：概率回流到其余 token，鼓励模型尝试其他潜在正确路线。

> **一句话：** NSR 不是教模型新知识，而是迫使模型 **回到已有正确知识** 上来。

---

### 5️⃣ Weighted-REINFORCE：实用合成策略

作者提出 **W-REINFORCE** 平衡两者：

$$\mathcal{L}_{W\text{-}REINFORCE}(\theta) = \lambda \cdot \mathcal{L}_{PSR}(\theta) + \mathcal{L}_{NSR}(\theta)$$

- $\lambda$ 调节正向奖励强度（实验中 $\lambda=0.1$ 最优）
- 结果：在所有数据集上 **Pass@k 全面最佳**，既保持高准确率又不丢多样性

---

### 6️⃣ 局限与注意事项

| 限制 | 影响 | 应对 |
|------|------|------|
| **长训练不稳定** | 几百步后性能回落，需要混合 PSR 稳定 | 采用 W-REINFORCE 或阶段性加入正样本 |
| **奖励过于二元** | 无法表达局部正确、过程奖励 | 结合分层 reward 或辅助监督信号 |
| **基座依赖强** | 弱模型上所有 RL 方法均退化 | 先用指令精调/监督强化提升基线 |

---

## 📐 数学与实现要点

### 1️⃣ NSR 梯度形式

对于错误样本 $y$：

$$\nabla_\theta \mathcal{L}_{NSR} = - \mathbb{E}_{y \sim \pi_\theta} \left[ \sum_t (1 - \pi_\theta(y_t|x, y_{<t})) \nabla_\theta \log \pi_\theta(y_t|x, y_{<t}) \right]$$

- **$(1-\pi)$ 因子**：当模型对错误 token 很自信时，梯度缩小，避免灾难性遗忘。
- **逐 token 惩罚**：只压制造成错误的 token，而不是整个序列。

### 2️⃣ 与 PSR 的互补

| 性质 | PSR | NSR |
|------|-----|-----|
| 目标 | 放大奖励路径 | 抑制错误路径 |
| 收敛速度 | 快 | 稳 |
| 多样性 | 容易丢失 | 保持甚至提升 |
| 先验保护 | 易破坏 | 自动保护 |

---

## 🎯 实战建议

- **什么时候用 NSR-only？**
  - 基座模型已有不错的 Pass@k（例如数学/逻辑任务）
  - 希望快速“去掉错误”而不破坏探索
  - 训练预算有限，难以维护正样本库

- **什么时候加 PSR？**
  - 追求最终极致 Pass@1
  - 长训练阶段需要稳定性
  - 存在多任务或过程奖励需求

- **实践 Tips：**
  - 准备高质量负样本：确保错误答案覆盖典型误区
  - 监控多样性指标（熵、Pass@k 曲线）判断是否需要引入 PSR
  - 与离线拒绝采样或自我纠错结合，形成“先惩后奖”流程

---

## 📊 快速复习卡片

| 知识点 | 核心内容 | 面试打法 |
|-------|----------|----------|
| **RLVR 拆解** | $ \mathcal{L}_{RLVR} = \mathcal{L}_{PSR} + \mathcal{L}_{NSR}$ | 强调正负信号分离的重要性 |
| **NSR 优势** | 精准惩罚错误、保持多样性 | 举例 Pass@k 曲线始终向上 |
| **关键洞察** | “告诉模型错了”比“告诉对了”更稳 | 结合梯度衰减解释先验保护 |
| **实用方案** | W-REINFORCE：$\lambda \mathcal{L}_{PSR} + \mathcal{L}_{NSR}$ | 说明如何落地 |
| **局限性** | 长期不稳定、奖励稀疏、基座依赖 | 主动提出风险与缓解策略 |

---

## 🔗 延伸阅读

- **Paper**：*The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning*（NeurIPS 2025）
- **RLVR 背景**：Reward 分解方法论
- **相关工作**：PPO / GRPO、Self-Rewarding、反事实奖励（CRR）
- **实践资源**：W-REINFORCE 代码示例、OpenRLHF NSR 配置参考

---

## 关注我，AI 不再难 🚀

