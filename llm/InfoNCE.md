# æ‰‹æ’•å¯¹æ¯”å­¦ä¹ loss

## ğŸ“Œ é¢è¯•æ ¸å¿ƒå›ç­”æ¡†æ¶

### ğŸ’¡ 30ç§’å¿«é€Ÿå›ç­”

> **æ ¸å¿ƒè§‚ç‚¹ï¼š** InfoNCE æ˜¯å¯¹æ¯”å­¦ä¹ çš„æ ¸å¿ƒæŸå¤±å‡½æ•°ï¼Œé€šè¿‡æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹çš„äº’ä¿¡æ¯ã€æœ€å°åŒ–è´Ÿæ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦ï¼Œå®ç°è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€‚å…³é”®åœ¨äºï¼š**â‘ æ¸©åº¦å‚æ•°æ§åˆ¶åˆ†å¸ƒå°–é”åº¦ â‘¡è´Ÿæ ·æœ¬æ•°é‡å½±å“æ€§èƒ½ â‘¢ä¸äº’ä¿¡æ¯ä¸‹ç•Œçš„å…³ç³»**ã€‚

---

## ä¸€ã€ç†è®ºåŸºç¡€ï¼šä» NCE åˆ° InfoNCE

### 1.1 ä¿¡æ¯è®ºåŸºç¡€ï¼šäº’ä¿¡æ¯ï¼ˆMutual Informationï¼‰

InfoNCE çš„æ ¸å¿ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹ä¹‹é—´çš„äº’ä¿¡æ¯ï¼š

```
I(x; y) = H(x) - H(x|y) = Î£ p(x,y) log(p(x|y) / p(x))
```

#### å…¬å¼ä¸­å„å˜é‡çš„è¯¦ç»†è§£é‡Š

**1. I(x; y) - äº’ä¿¡æ¯ï¼ˆMutual Informationï¼‰**
- **å®šä¹‰**ï¼šè¡¡é‡ä¸¤ä¸ªéšæœºå˜é‡ x å’Œ y ä¹‹é—´çš„ç›¸äº’ä¾èµ–ç¨‹åº¦
- **å•ä½**ï¼šæ¯”ç‰¹ï¼ˆbitsï¼‰æˆ–çº³ç‰¹ï¼ˆnatsï¼‰
- **å–å€¼èŒƒå›´**ï¼šI(x; y) â‰¥ 0
  - I(x; y) = 0ï¼šx å’Œ y ç›¸äº’ç‹¬ç«‹
  - I(x; y) > 0ï¼šx å’Œ y ç›¸å…³ï¼Œå€¼è¶Šå¤§ç›¸å…³æ€§è¶Šå¼º
- **å¯¹ç§°æ€§**ï¼šI(x; y) = I(y; x)

**2. H(x) - ç†µï¼ˆEntropyï¼‰**
- **å®šä¹‰**ï¼šéšæœºå˜é‡ x çš„ä¸ç¡®å®šæ€§ï¼ˆä¿¡æ¯é‡ï¼‰
- **å…¬å¼**ï¼šH(x) = -Î£ p(x) log p(x)
- **ç›´è§‚ç†è§£**ï¼š
  - ç†µè¶Šå¤§ï¼Œx çš„ä¸ç¡®å®šæ€§è¶Šå¤§ï¼Œä¿¡æ¯é‡è¶Šå¤š
  - ç†µè¶Šå°ï¼Œx è¶Šç¡®å®šï¼Œä¿¡æ¯é‡è¶Šå°‘
- **ä¾‹å­**ï¼š
  - æŠ›ç¡¬å¸ï¼šH(å…¬å¹³ç¡¬å¸) = 1 bitï¼ŒH(æ€»æ˜¯æ­£é¢) = 0 bit

**3. H(x|y) - æ¡ä»¶ç†µï¼ˆConditional Entropyï¼‰**
- **å®šä¹‰**ï¼šåœ¨å·²çŸ¥ y çš„æ¡ä»¶ä¸‹ï¼Œx çš„ä¸ç¡®å®šæ€§
- **å…¬å¼**ï¼šH(x|y) = -Î£ p(x,y) log p(x|y)
- **ç›´è§‚ç†è§£**ï¼š
  - H(x|y) è¡¨ç¤º"çŸ¥é“ y åï¼Œx è¿˜å‰©ä¸‹å¤šå°‘ä¸ç¡®å®šæ€§"
  - å¦‚æœ y å®Œå…¨å†³å®š xï¼Œåˆ™ H(x|y) = 0
  - å¦‚æœ y å’Œ x æ— å…³ï¼Œåˆ™ H(x|y) = H(x)

**4. p(x, y) - è”åˆæ¦‚ç‡ï¼ˆJoint Probabilityï¼‰**
- **å®šä¹‰**ï¼šx å’Œ y åŒæ—¶å‘ç”Ÿçš„æ¦‚ç‡
- **ä¾‹å­**ï¼šp(ä¸‹é›¨, å¸¦ä¼) = 0.3 è¡¨ç¤º"ä¸‹é›¨ä¸”å¸¦ä¼"çš„æ¦‚ç‡æ˜¯ 30%

**5. p(x|y) - æ¡ä»¶æ¦‚ç‡ï¼ˆConditional Probabilityï¼‰**
- **å®šä¹‰**ï¼šåœ¨ y å‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼Œx å‘ç”Ÿçš„æ¦‚ç‡
- **å…¬å¼**ï¼šp(x|y) = p(x,y) / p(y)
- **ä¾‹å­**ï¼šp(å¸¦ä¼|ä¸‹é›¨) = 0.8 è¡¨ç¤º"å¦‚æœä¸‹é›¨ï¼Œå¸¦ä¼çš„æ¦‚ç‡æ˜¯ 80%"

**6. p(x) - è¾¹ç¼˜æ¦‚ç‡ï¼ˆMarginal Probabilityï¼‰**
- **å®šä¹‰**ï¼šx å‘ç”Ÿçš„æ¦‚ç‡ï¼ˆä¸è€ƒè™‘ yï¼‰
- **å…¬å¼**ï¼šp(x) = Î£_y p(x,y)
- **ä¾‹å­**ï¼šp(å¸¦ä¼) = 0.5 è¡¨ç¤º"å¸¦ä¼çš„æ¦‚ç‡æ˜¯ 50%"

#### äº’ä¿¡æ¯çš„ä¸‰ç§ç­‰ä»·å½¢å¼

**å½¢å¼1ï¼šç†µçš„å·®**
```
I(x; y) = H(x) - H(x|y)
```
- **å«ä¹‰**ï¼šx çš„ä¸ç¡®å®šæ€§ - çŸ¥é“ y å x çš„ä¸ç¡®å®šæ€§ = x å’Œ y çš„äº’ä¿¡æ¯
- **ç›´è§‚ç†è§£**ï¼šy æä¾›äº†å¤šå°‘å…³äº x çš„ä¿¡æ¯

**å½¢å¼2ï¼šå¯¹ç§°å½¢å¼**
```
I(x; y) = H(x) + H(y) - H(x, y)
```
- **å«ä¹‰**ï¼šx çš„ä¿¡æ¯ + y çš„ä¿¡æ¯ - è”åˆä¿¡æ¯ = äº’ä¿¡æ¯
- **ç›´è§‚ç†è§£**ï¼šäº’ä¿¡æ¯æ˜¯"å…±äº«çš„ä¿¡æ¯"

**å½¢å¼3ï¼šKL æ•£åº¦å½¢å¼ï¼ˆæœ€å¸¸ç”¨ï¼‰**
```
I(x; y) = Î£ p(x,y) log(p(x|y) / p(x))
        = Î£ p(x,y) log(p(x,y) / (p(x) p(y)))
        = KL(p(x,y) || p(x) p(y))
```
- **å«ä¹‰**ï¼šè”åˆåˆ†å¸ƒä¸ç‹¬ç«‹åˆ†å¸ƒçš„ KL æ•£åº¦
- **ç›´è§‚ç†è§£**ï¼šè¡¡é‡"å®é™…åˆ†å¸ƒ"ä¸"å‡è®¾ç‹¬ç«‹"çš„å·®å¼‚

#### å…·ä½“ä¾‹å­

**ä¾‹å­1ï¼šå®Œå…¨ç›¸å…³**
```
x = yï¼ˆå®Œå…¨ç›¸å…³ï¼‰
- p(x=0, y=0) = 0.5, p(x=1, y=1) = 0.5
- p(x|y) = 1ï¼ˆå¦‚æœ y=0ï¼Œåˆ™ x ä¸€å®šæ˜¯ 0ï¼‰
- H(x|y) = 0ï¼ˆçŸ¥é“ y åï¼Œx å®Œå…¨ç¡®å®šï¼‰
- I(x; y) = H(x) - 0 = H(x) = 1 bitï¼ˆæœ€å¤§äº’ä¿¡æ¯ï¼‰
```

**ä¾‹å­2ï¼šå®Œå…¨ç‹¬ç«‹**
```
x å’Œ y ç‹¬ç«‹
- p(x,y) = p(x) p(y)
- p(x|y) = p(x)ï¼ˆçŸ¥é“ y ä¸å½±å“ x çš„åˆ†å¸ƒï¼‰
- H(x|y) = H(x)ï¼ˆæ¡ä»¶ç†µç­‰äºæ— æ¡ä»¶ç†µï¼‰
- I(x; y) = H(x) - H(x) = 0ï¼ˆæ— äº’ä¿¡æ¯ï¼‰
```

**ä¾‹å­3ï¼šéƒ¨åˆ†ç›¸å…³**
```
x æ˜¯"å¤©æ°”"ï¼ˆæ™´/é›¨ï¼‰ï¼Œy æ˜¯"å¸¦ä¼"ï¼ˆæ˜¯/å¦ï¼‰
- p(æ™´, å¸¦ä¼) = 0.2
- p(æ™´, ä¸å¸¦ä¼) = 0.5
- p(é›¨, å¸¦ä¼) = 0.3
- p(é›¨, ä¸å¸¦ä¼) = 0.0

è®¡ç®—ï¼š
- H(x) = -0.7*log(0.7) - 0.3*log(0.3) â‰ˆ 0.88 bit
- H(x|y=å¸¦ä¼) = -0.4*log(0.4) - 0.6*log(0.6) â‰ˆ 0.97 bit
- H(x|y=ä¸å¸¦ä¼) = -1*log(1) - 0*log(0) = 0 bit
- H(x|y) = 0.5*0.97 + 0.5*0 = 0.485 bit
- I(x; y) = 0.88 - 0.485 = 0.395 bit
```

#### åœ¨å¯¹æ¯”å­¦ä¹ ä¸­çš„åº”ç”¨

åœ¨å¯¹æ¯”å­¦ä¹ ä¸­ï¼š
- **x**ï¼šåŸå§‹æ ·æœ¬çš„è¡¨ç¤º z
- **y**ï¼šæ­£æ ·æœ¬çš„è¡¨ç¤º z^+
- **ç›®æ ‡**ï¼šæœ€å¤§åŒ– I(z; z^+)

**ä¸ºä»€ä¹ˆæœ€å¤§åŒ–äº’ä¿¡æ¯ï¼Ÿ**
- I(z; z^+) å¤§ â†’ z å’Œ z^+ é«˜åº¦ç›¸å…³
- è¯´æ˜æ¨¡å‹å­¦ä¹ åˆ°äº†æœ‰æ„ä¹‰çš„è¡¨ç¤º
- æ­£æ ·æœ¬å¯¹ï¼ˆåŒä¸€å›¾åƒçš„ä¸åŒå¢å¼ºï¼‰åº”è¯¥ç›¸ä¼¼

**äº’ä¿¡æ¯çš„ç›´è§‚ç†è§£ï¼š**
- I(z; z^+) = 0ï¼šz å’Œ z^+ å®Œå…¨æ— å…³ï¼ˆå­¦ä¹ å¤±è´¥ï¼‰
- I(z; z^+) å¤§ï¼šz å’Œ z^+ é«˜åº¦ç›¸å…³ï¼ˆå­¦ä¹ æˆåŠŸï¼‰
- é€šè¿‡æœ€å¤§åŒ–äº’ä¿¡æ¯ï¼Œæ¨¡å‹å­¦ä¹ åˆ°"è¯­ä¹‰ç›¸ä¼¼"çš„è¡¨ç¤º

#### äº’ä¿¡æ¯çš„å¯è§†åŒ–ç†è§£

```
ä¿¡æ¯è®ºè§†è§’ï¼ˆéŸ¦æ©å›¾ï¼‰ï¼š

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   H(x)          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚ I(x;y)   â”‚   â”‚  H(y)
    â”‚  â”‚          â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚   H(x|y)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

I(x; y) = H(x) - H(x|y)
        = H(y) - H(y|x)
        = H(x) + H(y) - H(x,y)
```

**å…³é”®å…³ç³»ï¼š**
- **H(x)**ï¼šx çš„æ€»ä¿¡æ¯é‡ï¼ˆå¤§åœ†ï¼‰
- **H(x|y)**ï¼šçŸ¥é“ y åï¼Œx è¿˜å‰©ä¸‹çš„ä¸ç¡®å®šæ€§ï¼ˆx åœ†ä¸­ä¸é‡å éƒ¨åˆ†ï¼‰
- **I(x; y)**ï¼šx å’Œ y å…±äº«çš„ä¿¡æ¯ï¼ˆé‡å éƒ¨åˆ†ï¼‰
- **H(x, y)**ï¼šx å’Œ y çš„è”åˆä¿¡æ¯é‡ï¼ˆä¸¤ä¸ªåœ†çš„å¹¶é›†ï¼‰

**åœ¨å¯¹æ¯”å­¦ä¹ ä¸­çš„å¯¹åº”ï¼š**
- H(z)ï¼šåŸå§‹è¡¨ç¤º z çš„ä¿¡æ¯é‡
- H(z|z^+)ï¼šçŸ¥é“æ­£æ ·æœ¬ z^+ åï¼Œz è¿˜å‰©ä¸‹çš„ä¸ç¡®å®šæ€§
- I(z; z^+)ï¼šz å’Œ z^+ å…±äº«çš„è¯­ä¹‰ä¿¡æ¯ï¼ˆæˆ‘ä»¬æƒ³æœ€å¤§åŒ–çš„éƒ¨åˆ†ï¼‰

### 1.2 Noise Contrastive Estimation (NCE)

NCE çš„æ ¸å¿ƒæ€æƒ³ï¼š**å°†å¯†åº¦ä¼°è®¡é—®é¢˜è½¬åŒ–ä¸ºäºŒåˆ†ç±»é—®é¢˜**

```
åŸå§‹é—®é¢˜ï¼šä¼°è®¡ p(x)ï¼ˆå›°éš¾ï¼‰
è½¬åŒ–é—®é¢˜ï¼šåŒºåˆ†çœŸå®æ ·æœ¬ x å’Œå™ªå£°æ ·æœ¬ x~ï¼ˆç®€å•ï¼‰
```

NCE æŸå¤±ï¼š
```
L_NCE = -log(Ïƒ(f(x) - log(p_n(x)))) - Î£ log(1 - Ïƒ(f(x~) - log(p_n(x~))))
```

å…¶ä¸­ï¼š
- `f(x)` æ˜¯æ¨¡å‹è¾“å‡ºçš„åˆ†æ•°
- `p_n(x)` æ˜¯å™ªå£°åˆ†å¸ƒ
- `Ïƒ` æ˜¯ sigmoid å‡½æ•°

### 1.3 InfoNCEï¼šä» NCE åˆ°å¯¹æ¯”å­¦ä¹ 

InfoNCE å°† NCE æ‰©å±•åˆ°å¤šåˆ†ç±»é—®é¢˜ï¼š

```
L_InfoNCE = -log(exp(sim(z, z^+) / Ï„) / Î£_{j=1}^{N} exp(sim(z, z_j) / Ï„))
```

**å…³é”®æ”¹è¿›ï¼š**
1. **å¤šè´Ÿæ ·æœ¬**ï¼šä»äºŒåˆ†ç±»æ‰©å±•åˆ° N åˆ†ç±»ï¼ˆ1 æ­£ + N-1 è´Ÿï¼‰
2. **æ¸©åº¦å‚æ•° Ï„**ï¼šæ§åˆ¶åˆ†å¸ƒçš„å°–é”ç¨‹åº¦
3. **äº’ä¿¡æ¯ä¸‹ç•Œ**ï¼šInfoNCE æ˜¯äº’ä¿¡æ¯çš„ä¸‹ç•Œï¼ˆè¯æ˜è§ä¸‹æ–‡ï¼‰

### 1.4 äº’ä¿¡æ¯ä¸‹ç•Œè¯æ˜ï¼ˆé¢è¯•é‡ç‚¹ï¼‰

**å®šç†ï¼š** InfoNCE æ˜¯äº’ä¿¡æ¯ I(z; z^+) çš„ä¸‹ç•Œ

**è¯æ˜æ€è·¯ï¼š**

```
L_InfoNCE = -E[log(exp(sim(z, z^+) / Ï„) / Î£_j exp(sim(z, z_j) / Ï„))]

å±•å¼€åï¼š
= -E[sim(z, z^+) / Ï„] + E[log Î£_j exp(sim(z, z_j) / Ï„)]

æ ¹æ® Jensen ä¸ç­‰å¼ï¼š
â‰¥ -E[sim(z, z^+) / Ï„] + log E[Î£_j exp(sim(z, z_j) / Ï„)]

å½“ f(z, z^+) = sim(z, z^+) / Ï„ æ˜¯äº’ä¿¡æ¯çš„è¯„åˆ†å‡½æ•°æ—¶ï¼š
â‰¥ -I(z; z^+) + log N

å› æ­¤ï¼š
I(z; z^+) â‰¥ log N - L_InfoNCE
```

**é¢è¯•è¦ç‚¹ï¼š**
- InfoNCE æ˜¯äº’ä¿¡æ¯çš„**ä¸‹ç•Œ**ï¼Œä¸æ˜¯ç²¾ç¡®å€¼
- è´Ÿæ ·æœ¬æ•°é‡ N è¶Šå¤§ï¼Œä¸‹ç•Œè¶Šç´§ï¼ˆtightï¼‰
- è¿™æ˜¯ä¸ºä»€ä¹ˆéœ€è¦å¤§é‡è´Ÿæ ·æœ¬çš„ç†è®ºä¾æ®

---

## äºŒã€æ•°å­¦å…¬å¼è¯¦è§£

### 2.1 æ ‡å‡†å…¬å¼

```
L_InfoNCE = -log(exp(sim(z_i, z_i^+) / Ï„) / Î£_{j=1}^{N} exp(sim(z_i, z_j) / Ï„))
```

**ç¬¦å·è¯´æ˜ï¼š**
- `z_i`ï¼šé”šç‚¹æ ·æœ¬ï¼ˆanchorï¼‰çš„è¡¨ç¤ºå‘é‡ [dim]
- `z_i^+`ï¼šæ­£æ ·æœ¬ï¼ˆpositiveï¼‰çš„è¡¨ç¤ºå‘é‡ [dim]
- `z_j`ï¼šè´Ÿæ ·æœ¬ï¼ˆnegativeï¼‰çš„è¡¨ç¤ºå‘é‡ [dim]ï¼Œj âˆˆ {1, 2, ..., N-1}
- `sim(Â·, Â·)`ï¼šç›¸ä¼¼åº¦å‡½æ•°ï¼Œé€šå¸¸æ˜¯å½’ä¸€åŒ–åçš„ç‚¹ç§¯ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
- `Ï„`ï¼šæ¸©åº¦å‚æ•°ï¼ˆtemperatureï¼‰ï¼Œæ§åˆ¶åˆ†å¸ƒçš„å°–é”ç¨‹åº¦
- `N`ï¼šæ€»æ ·æœ¬æ•°ï¼ˆ1 æ­£ + N-1 è´Ÿï¼‰

### 2.2 ç›¸ä¼¼åº¦å‡½æ•°çš„é€‰æ‹©

**1. ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæœ€å¸¸ç”¨ï¼‰**
```
sim(z_i, z_j) = z_i^T z_j / (||z_i|| ||z_j||) = z_i^T z_j  (å½’ä¸€åŒ–å)
```

**2. ç‚¹ç§¯ï¼ˆéœ€å½’ä¸€åŒ–ï¼‰**
```
sim(z_i, z_j) = z_i^T z_j  (è¦æ±‚ ||z_i|| = ||z_j|| = 1)
```

**3. æ¬§æ°è·ç¦»ï¼ˆè¾ƒå°‘ç”¨ï¼‰**
```
sim(z_i, z_j) = -||z_i - z_j||^2
```

**ä¸ºä»€ä¹ˆé€‰æ‹©ä½™å¼¦ç›¸ä¼¼åº¦ï¼Ÿ**
- å½’ä¸€åŒ–åï¼Œç›¸ä¼¼åº¦åœ¨ [-1, 1] èŒƒå›´å†…ï¼Œæ•°å€¼ç¨³å®š
- ä¸å—å‘é‡é•¿åº¦å½±å“ï¼Œåªå…³æ³¨æ–¹å‘
- è®¡ç®—é«˜æ•ˆï¼ˆç‚¹ç§¯æ“ä½œï¼‰

### 2.3 æ¸©åº¦å‚æ•° Ï„ çš„æ•°å­¦æ„ä¹‰

æ¸©åº¦å‚æ•°æ§åˆ¶ softmax åˆ†å¸ƒçš„ç†µï¼š

```
P(z^+|z) = exp(sim(z, z^+) / Ï„) / Î£_j exp(sim(z, z_j) / Ï„)
```

**Ï„ çš„å½±å“ï¼š**
- **Ï„ â†’ 0**ï¼šåˆ†å¸ƒè¶‹äº one-hotï¼Œåªå…³æ³¨æœ€ç›¸ä¼¼çš„æ ·æœ¬
- **Ï„ â†’ âˆ**ï¼šåˆ†å¸ƒè¶‹äºå‡åŒ€ï¼Œæ‰€æœ‰æ ·æœ¬æƒé‡ç›¸ç­‰
- **Ï„ = 1**ï¼šæ ‡å‡† softmax

**ç»éªŒå€¼ï¼š**
- SimCLR: Ï„ = 0.07
- MoCo: Ï„ = 0.07
- CLIP: Ï„ = 0.01 (å¯å­¦ä¹ )

**é¢è¯•é—®é¢˜ï¼šä¸ºä»€ä¹ˆ Ï„ è¿™ä¹ˆå°ï¼Ÿ**
- å°æ¸©åº¦ä½¿æ¨¡å‹æ›´å…³æ³¨**å›°éš¾è´Ÿæ ·æœ¬**ï¼ˆhard negativesï¼‰
- æé«˜è¡¨ç¤ºå­¦ä¹ çš„åˆ¤åˆ«èƒ½åŠ›
- ä½†è¿‡å°ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®š

### 2.4 æ‰¹æ¬¡å†…è´Ÿæ ·æœ¬ï¼ˆIn-batch Negativesï¼‰

åœ¨å®é™…å®ç°ä¸­ï¼Œé€šå¸¸ä½¿ç”¨æ‰¹æ¬¡å†…å…¶ä»–æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬ï¼š

```
å¯¹äºæ‰¹æ¬¡å¤§å° batch_size = 2Nï¼š
- æ¯ä¸ªæ ·æœ¬æœ‰ 1 ä¸ªæ­£æ ·æœ¬ï¼ˆå¦ä¸€ä¸ªå¢å¼ºç‰ˆæœ¬ï¼‰
- æ¯ä¸ªæ ·æœ¬æœ‰ 2N-2 ä¸ªè´Ÿæ ·æœ¬ï¼ˆæ‰¹æ¬¡å†…å…¶ä»–æ ·æœ¬ï¼‰
```

**ä¼˜åŠ¿ï¼š**
- æ— éœ€é¢å¤–å­˜å‚¨è´Ÿæ ·æœ¬
- è®¡ç®—é«˜æ•ˆï¼ˆä¸€æ¬¡çŸ©é˜µä¹˜æ³•ï¼‰
- è´Ÿæ ·æœ¬å¤šæ ·æ€§å¥½

**åŠ£åŠ¿ï¼š**
- è´Ÿæ ·æœ¬æ•°é‡å—æ‰¹æ¬¡å¤§å°é™åˆ¶
- å¯èƒ½å­˜åœ¨å‡è´Ÿæ ·æœ¬ï¼ˆfalse negativesï¼‰

---

## ä¸‰ã€å®Œæ•´å®ç°ï¼ˆç”Ÿäº§çº§ä»£ç ï¼‰

### 3.1 æ ‡å‡†å®ç°ï¼šSimCLR é£æ ¼ï¼ˆæ¨èï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional


class InfoNCE(nn.Module):
    """
    InfoNCE å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼ˆç”Ÿäº§çº§å®ç°ï¼‰
    
    é€‚ç”¨äº SimCLR åœºæ™¯ï¼šè¾“å…¥ [2*N, dim]ï¼Œæ¯ä¸¤ä¸ªè¿ç»­æ ·æœ¬æ˜¯æ­£æ ·æœ¬å¯¹
    ä¾‹å¦‚ï¼š[z1, z1', z2, z2', ...] å…¶ä¸­ (z1, z1') æ˜¯æ­£æ ·æœ¬å¯¹
    
    Args:
        temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ 0.07
        reduction: æŸå¤±èšåˆæ–¹å¼ï¼Œ'mean' æˆ– 'sum'
    
    Reference:
        - SimCLR: https://arxiv.org/abs/2002.05709
        - CPC: https://arxiv.org/abs/1807.03748
    """
    def __init__(self, temperature: float = 0.07, reduction: str = 'mean'):
        super().__init__()
        assert temperature > 0, "Temperature must be positive"
        assert reduction in ['mean', 'sum', 'none'], "Reduction must be 'mean', 'sum', or 'none'"
        
        self.temperature = temperature
        self.reduction = reduction
    
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®— InfoNCE æŸå¤±
        
        Args:
            features: [2*N, dim] ç‰¹å¾å‘é‡ï¼Œæ¯ä¸¤ä¸ªè¿ç»­æ ·æœ¬æ˜¯æ­£æ ·æœ¬å¯¹
                    ä¾‹å¦‚ï¼š[z1, z1', z2, z2', ...]
        
        Returns:
            loss: InfoNCE æŸå¤±å€¼ï¼ˆæ ‡é‡æˆ– [2*N]ï¼‰
        
        Shape:
            - Input: [2*N, dim]
            - Output: scalar (if reduction='mean') or [2*N] (if reduction='none')
        """
        batch_size, dim = features.shape
        assert batch_size % 2 == 0, f"Batch size must be even, got {batch_size}"
        
        device = features.device
        n = batch_size // 2
        
        # L2 å½’ä¸€åŒ–ï¼ˆå…³é”®æ­¥éª¤ï¼‰
        features = F.normalize(features, p=2, dim=1, eps=1e-8)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ [2*N, 2*N]
        # similarity_matrix[i, j] = features[i]^T @ features[j]
        similarity_matrix = torch.matmul(features, features.t())  # [2*N, 2*N]
        
        # åˆ›å»ºæ­£æ ·æœ¬å¯¹æ©ç 
        # æ­£æ ·æœ¬å¯¹ï¼šç´¢å¼• (0,1), (2,3), (4,5), ...
        # mask[i, j] = True è¡¨ç¤º (i, j) æ˜¯æ­£æ ·æœ¬å¯¹
        mask = torch.zeros(batch_size, batch_size, dtype=torch.bool, device=device)
        for i in range(0, batch_size, 2):
            mask[i, i+1] = True
            mask[i+1, i] = True
        
        # æå–æ­£æ ·æœ¬ç›¸ä¼¼åº¦ [2*N]
        pos_sim = similarity_matrix[mask].unsqueeze(1)  # [2*N, 1]
        
        # åˆ›å»ºè´Ÿæ ·æœ¬æ©ç ï¼ˆæ’é™¤è‡ªå·±å’Œæ­£æ ·æœ¬ï¼‰
        neg_mask = ~mask
        neg_mask.fill_diagonal_(False)  # æ’é™¤è‡ªå·±
        
        # åº”ç”¨æ¸©åº¦å‚æ•°
        similarity_matrix = similarity_matrix / self.temperature
        
        # æ„å»º logitsï¼š[æ­£æ ·æœ¬ç›¸ä¼¼åº¦, è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦1, è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦2, ...]
        # æ–¹æ³•ï¼šå°†æ­£æ ·æœ¬ä½ç½®å’Œå¯¹è§’çº¿è®¾ä¸º -infï¼Œç„¶åæå–è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        logits = similarity_matrix.clone()
        logits[mask] = float('-inf')  # æ’é™¤æ­£æ ·æœ¬ä½ç½®
        logits.fill_diagonal_(float('-inf'))  # æ’é™¤è‡ªå·±
        
        # æå–è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦ [2*N, 2*N-2]
        neg_sim = logits[neg_mask].reshape(batch_size, -1)
        
        # åˆå¹¶æ­£è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦ [2*N, 2*N-1]
        logits = torch.cat([pos_sim, neg_sim], dim=1)
        
        # æ ‡ç­¾ï¼šç¬¬ä¸€ä¸ªä½ç½®ï¼ˆç´¢å¼• 0ï¼‰æ˜¯æ­£æ ·æœ¬
        labels = torch.zeros(batch_size, dtype=torch.long, device=device)
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼ˆç­‰ä»·äº InfoNCEï¼‰
        loss = F.cross_entropy(logits, labels, reduction=self.reduction)
        
        return loss
    
    def extra_repr(self) -> str:
        return f'temperature={self.temperature}, reduction={self.reduction}'
```

### 3.2 é«˜æ•ˆå®ç°ï¼šé¿å…å®Œæ•´ç›¸ä¼¼åº¦çŸ©é˜µ

```python
class InfoNCEOptimized(nn.Module):
    """
    å†…å­˜ä¼˜åŒ–çš„ InfoNCE å®ç°
    é€‚ç”¨äºå¤§æ‰¹æ¬¡åœºæ™¯ï¼Œé¿å…æ„å»ºå®Œæ•´çš„ [2*N, 2*N] ç›¸ä¼¼åº¦çŸ©é˜µ
    
    å…³é”®ä¼˜åŒ–ï¼š
    1. åˆ†ç¦»æ­£æ ·æœ¬å¯¹ï¼Œåˆ†åˆ«è®¡ç®—
    2. ä½¿ç”¨åˆ†æ‰¹è®¡ç®—å‡å°‘å†…å­˜å ç”¨
    """
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        assert temperature > 0, "Temperature must be positive"
        self.temperature = temperature
    
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            features: [2*N, dim] ç‰¹å¾å‘é‡
        
        Returns:
            loss: InfoNCE æŸå¤±å€¼
        """
        batch_size, dim = features.shape
        assert batch_size % 2 == 0, f"Batch size must be even, got {batch_size}"
        
        device = features.device
        n = batch_size // 2
        
        # L2 å½’ä¸€åŒ–
        features = F.normalize(features, p=2, dim=1, eps=1e-8)
        
        # åˆ†ç¦»æ­£æ ·æœ¬å¯¹
        z1 = features[0::2]  # [N, dim] ç¬¬ä¸€ä¸ªå¢å¼ºç‰ˆæœ¬
        z2 = features[1::2]  # [N, dim] ç¬¬äºŒä¸ªå¢å¼ºç‰ˆæœ¬
        
        # è®¡ç®—æ­£æ ·æœ¬ç›¸ä¼¼åº¦ [N]
        pos_sim = torch.sum(z1 * z2, dim=1, keepdim=True) / self.temperature  # [N, 1]
        
        # è®¡ç®— z1 ä¸æ‰€æœ‰ features çš„ç›¸ä¼¼åº¦ [N, 2*N]
        all_sim_z1 = torch.matmul(z1, features.t()) / self.temperature
        # è®¡ç®— z2 ä¸æ‰€æœ‰ features çš„ç›¸ä¼¼åº¦ [N, 2*N]
        all_sim_z2 = torch.matmul(z2, features.t()) / self.temperature
        
        # åˆ›å»ºæ©ç ï¼šæ’é™¤è‡ªå·±å’Œå¯¹åº”çš„æ­£æ ·æœ¬
        # å¯¹äº z1[i]ï¼Œæ’é™¤ z1[i]ï¼ˆç´¢å¼• 2*iï¼‰å’Œ z2[i]ï¼ˆç´¢å¼• 2*i+1ï¼Œæ­£æ ·æœ¬ï¼‰
        mask_z1 = torch.zeros(n, 2*n, dtype=torch.bool, device=device)
        mask_z2 = torch.zeros(n, 2*n, dtype=torch.bool, device=device)
        
        for i in range(n):
            mask_z1[i, 2*i] = True      # æ’é™¤ z1[i]ï¼ˆè‡ªå·±ï¼‰
            mask_z1[i, 2*i+1] = True    # æ’é™¤ z2[i]ï¼ˆæ­£æ ·æœ¬ï¼‰
            mask_z2[i, 2*i] = True      # æ’é™¤ z1[i]ï¼ˆæ­£æ ·æœ¬ï¼‰
            mask_z2[i, 2*i+1] = True    # æ’é™¤ z2[i]ï¼ˆè‡ªå·±ï¼‰
        
        # åº”ç”¨æ©ç 
        all_sim_z1 = all_sim_z1.masked_fill(mask_z1, float('-inf'))
        all_sim_z2 = all_sim_z2.masked_fill(mask_z2, float('-inf'))
        
        # æ„å»º logitsï¼šæ­£æ ·æœ¬ç›¸ä¼¼åº¦ + è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        logits_z1 = torch.cat([pos_sim, all_sim_z1], dim=1)  # [N, 2*N]
        logits_z2 = torch.cat([pos_sim, all_sim_z2], dim=1)  # [N, 2*N]
        
        # åˆå¹¶æ‰€æœ‰ logits
        logits = torch.cat([logits_z1, logits_z2], dim=0)  # [2*N, 2*N]
        
        # æ ‡ç­¾ï¼šç¬¬ä¸€ä¸ªä½ç½®æ˜¯æ­£æ ·æœ¬
        labels = torch.zeros(2*n, dtype=torch.long, device=device)
        
        # è®¡ç®—æŸå¤±
        loss = F.cross_entropy(logits, labels)
        
        return loss
```

### 3.3 å¯¹ç§° InfoNCEï¼ˆåŒå‘æŸå¤±ï¼‰

```python
class SymmetricInfoNCE(nn.Module):
    """
    å¯¹ç§° InfoNCEï¼šåŒæ—¶è®¡ç®—ä¸¤ä¸ªæ–¹å‘çš„æŸå¤±
    L = (L(z1->z2) + L(z2->z1)) / 2
    
    ä¼˜åŠ¿ï¼š
    - æ›´å¯¹ç§°çš„ä¼˜åŒ–ç›®æ ‡
    - åœ¨æŸäº›ä»»åŠ¡ä¸Šæ•ˆæœæ›´å¥½
    """
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, z1: torch.Tensor, z2: torch.Tensor) -> torch.Tensor:
        """
        Args:
            z1, z2: [batch_size, dim] ä¸¤ä¸ªå¢å¼ºç‰ˆæœ¬çš„ç‰¹å¾
        
        Returns:
            loss: å¯¹ç§° InfoNCE æŸå¤±å€¼
        """
        batch_size = z1.size(0)
        device = z1.device
        
        # å½’ä¸€åŒ–
        z1 = F.normalize(z1, p=2, dim=1, eps=1e-8)
        z2 = F.normalize(z2, p=2, dim=1, eps=1e-8)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ [batch_size, batch_size]
        sim_matrix = torch.matmul(z1, z2.t()) / self.temperature
        
        # æ­£æ ·æœ¬å¯¹åœ¨å¯¹è§’çº¿ä¸Š
        labels = torch.arange(batch_size, device=device)
        
        # ä¸¤ä¸ªæ–¹å‘çš„æŸå¤±
        loss_12 = F.cross_entropy(sim_matrix, labels)
        loss_21 = F.cross_entropy(sim_matrix.t(), labels)
        
        # å¯¹ç§°æŸå¤±
        loss = (loss_12 + loss_21) / 2
        
        return loss
```

### 3.4 æ•°å€¼ç¨³å®šæ€§ä¼˜åŒ–

```python
class InfoNCENumericallyStable(nn.Module):
    """
    æ•°å€¼ç¨³å®šçš„ InfoNCE å®ç°
    ä½¿ç”¨ log-sum-exp æŠ€å·§é¿å…æ•°å€¼æº¢å‡º
    """
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        batch_size, dim = features.shape
        assert batch_size % 2 == 0
        
        device = features.device
        n = batch_size // 2
        
        # å½’ä¸€åŒ–
        features = F.normalize(features, p=2, dim=1, eps=1e-8)
        
        # åˆ†ç¦»æ­£æ ·æœ¬å¯¹
        z1 = features[0::2]  # [N, dim]
        z2 = features[1::2]  # [N, dim]
        
        # è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦
        all_sim = torch.matmul(features, features.t()) / self.temperature  # [2*N, 2*N]
        
        # æ­£æ ·æœ¬ç›¸ä¼¼åº¦ï¼ˆåœ¨å¯¹è§’çº¿ä¸Šï¼Œä½†éœ€è¦è°ƒæ•´ç´¢å¼•ï¼‰
        pos_sim = torch.sum(z1 * z2, dim=1) / self.temperature  # [N]
        
        # å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œè®¡ç®— log-sum-exp
        losses = []
        for i in range(n):
            # z1[i] çš„æŸå¤±
            pos = pos_sim[i]
            negs = []
            for j in range(2*n):
                if j != 2*i and j != 2*i+1:  # æ’é™¤è‡ªå·±å’Œæ­£æ ·æœ¬
                    negs.append(all_sim[2*i, j])
            negs = torch.stack(negs)
            
            # ä½¿ç”¨ log-sum-exp æŠ€å·§
            max_val = torch.max(torch.cat([pos.unsqueeze(0), negs]))
            log_sum_exp = max_val + torch.log(
                torch.exp(pos - max_val) + torch.sum(torch.exp(negs - max_val))
            )
            loss_i = log_sum_exp - pos
            losses.append(loss_i)
            
            # z2[i] çš„æŸå¤±ï¼ˆç±»ä¼¼å¤„ç†ï¼‰
            pos = pos_sim[i]
            negs = []
            for j in range(2*n):
                if j != 2*i and j != 2*i+1:
                    negs.append(all_sim[2*i+1, j])
            negs = torch.stack(negs)
            
            max_val = torch.max(torch.cat([pos.unsqueeze(0), negs]))
            log_sum_exp = max_val + torch.log(
                torch.exp(pos - max_val) + torch.sum(torch.exp(negs - max_val))
            )
            loss_i = log_sum_exp - pos
            losses.append(loss_i)
        
        return torch.stack(losses).mean()
```

---

## å››ã€æ¢¯åº¦åˆ†æï¼ˆé¢è¯•é‡ç‚¹ï¼‰

### 4.1 InfoNCE çš„æ¢¯åº¦å…¬å¼

å¯¹æ­£æ ·æœ¬ç›¸ä¼¼åº¦ `s^+ = sim(z, z^+) / Ï„` çš„æ¢¯åº¦ï¼š

```
âˆ‚L/âˆ‚s^+ = -1 + P(z^+|z) = -(1 - P(z^+|z))
```

å¯¹è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦ `s^- = sim(z, z^-) / Ï„` çš„æ¢¯åº¦ï¼š

```
âˆ‚L/âˆ‚s^- = P(z^-|z)
```

**ç›´è§‚ç†è§£ï¼š**
- æ­£æ ·æœ¬æ¢¯åº¦ï¼š**è´Ÿå€¼**ï¼Œæ¨åŠ¨æ­£æ ·æœ¬ç›¸ä¼¼åº¦å¢åŠ 
- è´Ÿæ ·æœ¬æ¢¯åº¦ï¼š**æ­£å€¼**ï¼Œæ¨åŠ¨è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦å‡å°‘
- æ¢¯åº¦å¤§å°ä¸ softmax æ¦‚ç‡æˆæ­£æ¯”

### 4.2 æ¸©åº¦å‚æ•°å¯¹æ¢¯åº¦çš„å½±å“

```
å½“ Ï„ å¾ˆå°æ—¶ï¼š
- P(z^+|z) â†’ 1ï¼ˆå¦‚æœæ­£æ ·æœ¬æœ€ç›¸ä¼¼ï¼‰
- âˆ‚L/âˆ‚s^+ â†’ 0ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰
- ä½†å›°éš¾è´Ÿæ ·æœ¬çš„æ¢¯åº¦å¾ˆå¤§

å½“ Ï„ å¾ˆå¤§æ—¶ï¼š
- P(z^+|z) â†’ 1/Nï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
- æ‰€æœ‰æ ·æœ¬çš„æ¢¯åº¦éƒ½è¾ƒå°
- è®­ç»ƒç¨³å®šä½†å­¦ä¹ æ…¢
```

**é¢è¯•é—®é¢˜ï¼šå¦‚ä½•é€‰æ‹©æ¸©åº¦å‚æ•°ï¼Ÿ**
1. ä» 0.1 å¼€å§‹ï¼Œæ ¹æ®éªŒè¯é›†è°ƒæ•´
2. è§‚å¯Ÿè®­ç»ƒæ›²çº¿ï¼šæŸå¤±æ˜¯å¦ä¸‹é™ã€æ˜¯å¦ç¨³å®š
3. è€ƒè™‘ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½

### 4.3 è´Ÿæ ·æœ¬æ•°é‡å¯¹æ¢¯åº¦çš„å½±å“

```
è´Ÿæ ·æœ¬æ•°é‡ N å¢åŠ ï¼š
- äº’ä¿¡æ¯ä¸‹ç•Œæ›´ç´§ï¼ˆç†è®ºä¼˜åŠ¿ï¼‰
- ä½†æ¯ä¸ªè´Ÿæ ·æœ¬çš„æ¢¯åº¦å˜å°ï¼ˆ1/Nï¼‰
- éœ€è¦æƒè¡¡è®¡ç®—æˆæœ¬å’Œæ€§èƒ½
```

---

## äº”ã€å¸¸è§å˜ä½“ä¸æ”¹è¿›

### 5.1 MoCo (Momentum Contrast)

```python
class MoCoLoss(nn.Module):
    """
    MoCo ä½¿ç”¨åŠ¨é‡æ›´æ–°çš„ç¼–ç å™¨å’Œé˜Ÿåˆ—ç»´æŠ¤è´Ÿæ ·æœ¬
    
    å…³é”®åˆ›æ–°ï¼š
    1. åŠ¨é‡ç¼–ç å™¨ï¼škey encoder ä½¿ç”¨åŠ¨é‡æ›´æ–°
    2. é˜Ÿåˆ—æœºåˆ¶ï¼šç»´æŠ¤å¤§é‡è´Ÿæ ·æœ¬ï¼ˆ65536ï¼‰
    3. è§£è€¦æ‰¹æ¬¡å¤§å°å’Œè´Ÿæ ·æœ¬æ•°é‡
    """
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, q: torch.Tensor, k: torch.Tensor, queue: torch.Tensor) -> torch.Tensor:
        """
        Args:
            q: æŸ¥è¯¢ç‰¹å¾ [batch_size, dim]ï¼Œæ¥è‡ª query encoder
            k: é”®ç‰¹å¾ [batch_size, dim]ï¼Œæ¥è‡ª momentum encoderï¼ˆæ­£æ ·æœ¬ï¼‰
            queue: è´Ÿæ ·æœ¬é˜Ÿåˆ— [queue_size, dim]
        """
        batch_size = q.size(0)
        device = q.device
        
        # å½’ä¸€åŒ–
        q = F.normalize(q, p=2, dim=1, eps=1e-8)
        k = F.normalize(k, p=2, dim=1, eps=1e-8)
        queue = F.normalize(queue, p=2, dim=1, eps=1e-8)
        
        # æ­£æ ·æœ¬ç›¸ä¼¼åº¦
        pos_sim = torch.sum(q * k, dim=1, keepdim=True) / self.temperature  # [batch_size, 1]
        
        # è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        neg_sim = torch.matmul(q, queue.t()) / self.temperature  # [batch_size, queue_size]
        
        # åˆå¹¶
        logits = torch.cat([pos_sim, neg_sim], dim=1)  # [batch_size, 1 + queue_size]
        
        # æ ‡ç­¾
        labels = torch.zeros(batch_size, dtype=torch.long, device=device)
        
        # æŸå¤±
        loss = F.cross_entropy(logits, labels)
        
        return loss
```

### 5.2 Hard Negative Mining

```python
class InfoNCEWithHardNegatives(nn.Module):
    """
    ä½¿ç”¨å›°éš¾è´Ÿæ ·æœ¬çš„ InfoNCE
    åªé€‰æ‹©æœ€ç›¸ä¼¼çš„è´Ÿæ ·æœ¬ï¼ˆtop-kï¼‰å‚ä¸è®¡ç®—
    """
    def __init__(self, temperature: float = 0.07, top_k: int = 10):
        super().__init__()
        self.temperature = temperature
        self.top_k = top_k
    
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        batch_size, dim = features.shape
        assert batch_size % 2 == 0
        
        device = features.device
        n = batch_size // 2
        
        # å½’ä¸€åŒ–
        features = F.normalize(features, p=2, dim=1, eps=1e-8)
        
        # åˆ†ç¦»æ­£æ ·æœ¬å¯¹
        z1 = features[0::2]
        z2 = features[1::2]
        
        # æ­£æ ·æœ¬ç›¸ä¼¼åº¦
        pos_sim = torch.sum(z1 * z2, dim=1, keepdim=True) / self.temperature
        
        # è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦
        all_sim = torch.matmul(z1, features.t()) / self.temperature  # [N, 2*N]
        
        # åˆ›å»ºæ©ç 
        mask = torch.zeros(n, 2*n, dtype=torch.bool, device=device)
        for i in range(n):
            mask[i, 2*i] = True      # æ’é™¤è‡ªå·±
            mask[i, 2*i+1] = True   # æ’é™¤æ­£æ ·æœ¬
        
        all_sim = all_sim.masked_fill(mask, float('-inf'))
        
        # é€‰æ‹© top-k å›°éš¾è´Ÿæ ·æœ¬
        neg_sim, _ = torch.topk(all_sim, k=min(self.top_k, 2*n-2), dim=1)
        
        # æ„å»º logits
        logits = torch.cat([pos_sim, neg_sim], dim=1)
        
        # æ ‡ç­¾
        labels = torch.zeros(n, dtype=torch.long, device=device)
        
        # æŸå¤±
        loss = F.cross_entropy(logits, labels)
        
        return loss
```

### 5.3 å¯å­¦ä¹ æ¸©åº¦å‚æ•°

```python
class InfoNCEWithLearnableTemperature(nn.Module):
    """
    å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°
    è®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜æ¸©åº¦
    """
    def __init__(self, init_temperature: float = 0.07):
        super().__init__()
        # ä½¿ç”¨ log ç©ºé—´ï¼Œç¡®ä¿æ¸©åº¦å§‹ç»ˆä¸ºæ­£
        self.log_temperature = nn.Parameter(torch.log(torch.tensor(init_temperature)))
    
    @property
    def temperature(self):
        return torch.exp(self.log_temperature)
    
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        # ä½¿ç”¨ self.temperature è€Œä¸æ˜¯å›ºå®šå€¼
        # ...ï¼ˆå®ç°åŒæ ‡å‡†ç‰ˆæœ¬ï¼‰
        pass
```

---

## å…­ã€é¢è¯•å¸¸è§é—®é¢˜ï¼ˆQ&Aï¼‰

### Q1: InfoNCE å’Œäº¤å‰ç†µæŸå¤±çš„å…³ç³»ï¼Ÿ

**A:** InfoNCE æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„äº¤å‰ç†µæŸå¤±ï¼š
- å°†å¯¹æ¯”å­¦ä¹ é—®é¢˜è½¬åŒ–ä¸ºå¤šåˆ†ç±»é—®é¢˜
- æ­£æ ·æœ¬ä½œä¸ºç±»åˆ« 0ï¼Œè´Ÿæ ·æœ¬ä½œä¸ºå…¶ä»–ç±»åˆ«
- ä½¿ç”¨æ¸©åº¦å‚æ•°æ§åˆ¶åˆ†å¸ƒçš„å°–é”ç¨‹åº¦

**ä»£ç éªŒè¯ï¼š**
```python
# InfoNCE
logits = [pos_sim, neg_sim1, neg_sim2, ...] / Ï„
labels = [0, 0, 0, ...]  # ç¬¬ä¸€ä¸ªä½ç½®æ˜¯æ­£æ ·æœ¬
loss = CrossEntropy(logits, labels)

# ç­‰ä»·äº
loss = -log(exp(pos_sim/Ï„) / Î£ exp(sim/Ï„))
```

### Q2: ä¸ºä»€ä¹ˆéœ€è¦æ¸©åº¦å‚æ•°ï¼Ÿ

**A:** ä¸‰ä¸ªåŸå› ï¼š
1. **æ•°å€¼ç¨³å®šæ€§**ï¼šé˜²æ­¢ softmax é¥±å’Œ
2. **æ¢¯åº¦å¹³è¡¡**ï¼šæ§åˆ¶æ­£è´Ÿæ ·æœ¬çš„æ¢¯åº¦å¤§å°
3. **å›°éš¾è´Ÿæ ·æœ¬**ï¼šå°æ¸©åº¦ä½¿æ¨¡å‹æ›´å…³æ³¨å›°éš¾è´Ÿæ ·æœ¬

**å®éªŒéªŒè¯ï¼š**
- Ï„ = 0.01: è®­ç»ƒä¸ç¨³å®šï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ
- Ï„ = 0.07: å¹³è¡¡ç‚¹ï¼ˆSimCLR ä½¿ç”¨ï¼‰
- Ï„ = 1.0: è®­ç»ƒç¨³å®šä½†å­¦ä¹ æ…¢

### Q3: è´Ÿæ ·æœ¬æ•°é‡å¦‚ä½•å½±å“æ€§èƒ½ï¼Ÿ

**A:** 
- **ç†è®º**ï¼šè´Ÿæ ·æœ¬è¶Šå¤šï¼Œäº’ä¿¡æ¯ä¸‹ç•Œè¶Šç´§
- **å®è·µ**ï¼šæ”¶ç›Šé€’å‡ï¼Œé€šå¸¸ 4096-8192 è¶³å¤Ÿ
- **è®¡ç®—**ï¼šè´Ÿæ ·æœ¬æ•°é‡çº¿æ€§å¢åŠ è®¡ç®—æˆæœ¬

**å®éªŒæ•°æ®ï¼ˆSimCLRï¼‰ï¼š**
| è´Ÿæ ·æœ¬æ•° | Top-1 Acc |
|---------|-----------|
| 256     | 60.0%     |
| 512     | 63.5%     |
| 1024    | 66.2%     |
| 4096    | 69.3%     |
| 8192    | 69.8%     |

### Q4: InfoNCE å’Œ Triplet Loss çš„åŒºåˆ«ï¼Ÿ

**A:** 

| ç»´åº¦ | InfoNCE | Triplet Loss |
|------|---------|--------------|
| è´Ÿæ ·æœ¬æ•° | å¤šä¸ªï¼ˆN-1ï¼‰ | 1 ä¸ª |
| ä¼˜åŒ–ç›®æ ‡ | æœ€å¤§åŒ–äº’ä¿¡æ¯ | æœ€å¤§åŒ–é—´éš” |
| æ¢¯åº¦ç‰¹æ€§ | æ‰€æœ‰è´Ÿæ ·æœ¬éƒ½æœ‰æ¢¯åº¦ | åªæœ‰å›°éš¾è´Ÿæ ·æœ¬æœ‰æ¢¯åº¦ |
| è®¡ç®—å¤æ‚åº¦ | O(N) | O(1) |

**ä»£ç å¯¹æ¯”ï¼š**
```python
# Triplet Loss
loss = max(0, margin + sim(anchor, negative) - sim(anchor, positive))

# InfoNCE
loss = -log(exp(sim(anchor, positive)/Ï„) / Î£ exp(sim(anchor, sample)/Ï„))
```

### Q5: å¦‚ä½•è§£å†³å‡è´Ÿæ ·æœ¬ï¼ˆFalse Negativesï¼‰é—®é¢˜ï¼Ÿ

**A:** ä¸‰ç§æ–¹æ³•ï¼š
1. **å¢åŠ æ‰¹æ¬¡å¤§å°**ï¼šå‡å°‘å‡è´Ÿæ ·æœ¬æ¯”ä¾‹
2. **ä½¿ç”¨ MoCo é˜Ÿåˆ—**ï¼šä»å†å²æ‰¹æ¬¡é‡‡æ ·è´Ÿæ ·æœ¬
3. **Debiased Contrastive Learning**ï¼šæ˜¾å¼å»ºæ¨¡å‡è´Ÿæ ·æœ¬

### Q6: InfoNCE çš„å±€é™æ€§ï¼Ÿ

**A:** 
1. **éœ€è¦å¤§é‡è´Ÿæ ·æœ¬**ï¼šè®¡ç®—æˆæœ¬é«˜
2. **å‡è´Ÿæ ·æœ¬é—®é¢˜**ï¼šæ‰¹æ¬¡å†…å¯èƒ½å­˜åœ¨ç›¸ä¼¼æ ·æœ¬
3. **æ¸©åº¦å‚æ•°æ•æ„Ÿ**ï¼šéœ€è¦ä»”ç»†è°ƒå‚
4. **ä¸é€‚ç”¨äºç”Ÿæˆä»»åŠ¡**ï¼šåªé€‚ç”¨äºè¡¨ç¤ºå­¦ä¹ 

---

## ä¸ƒã€å®è·µæŠ€å·§ä¸æœ€ä½³å®è·µ

### 7.1 æ•°æ®å¢å¼ºç­–ç•¥

```python
# SimCLR çš„æ•°æ®å¢å¼ºç»„åˆ
augmentations = [
    RandomResizedCrop(),
    RandomHorizontalFlip(),
    ColorJitter(),
    RandomGrayscale(),
    GaussianBlur(),
]
```

**å…³é”®ç‚¹ï¼š**
- å¢å¼ºè¦è¶³å¤Ÿå¼ºï¼Œä½†ä¸èƒ½ç ´åè¯­ä¹‰
- ä¸åŒä»»åŠ¡éœ€è¦ä¸åŒçš„å¢å¼ºç­–ç•¥

### 7.2 è®­ç»ƒæŠ€å·§

1. **å­¦ä¹ ç‡è°ƒåº¦**ï¼šä½¿ç”¨ warmup + cosine decay
2. **æ‰¹æ¬¡å¤§å°**ï¼šè¶Šå¤§è¶Šå¥½ï¼ˆå— GPU å†…å­˜é™åˆ¶ï¼‰
3. **è®­ç»ƒè½®æ•°**ï¼šé€šå¸¸éœ€è¦ 100-1000 è½®
4. **ç‰¹å¾ç»´åº¦**ï¼š128-512 ç»´é€šå¸¸è¶³å¤Ÿ

### 7.3 è¯„ä¼°æŒ‡æ ‡

```python
# çº¿æ€§è¯„ä¼°ï¼ˆLinear Evaluationï¼‰
# å†»ç»“ç¼–ç å™¨ï¼Œåªè®­ç»ƒåˆ†ç±»å¤´
classifier = nn.Linear(feature_dim, num_classes)
optimizer = optim.SGD(classifier.parameters(), lr=0.1)

# k-NN è¯„ä¼°
# ä½¿ç”¨ k-NN åˆ†ç±»å™¨è¯„ä¼°è¡¨ç¤ºè´¨é‡
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=20)
```

### 7.4 è°ƒè¯•æŠ€å·§

```python
# 1. æ£€æŸ¥ç‰¹å¾å½’ä¸€åŒ–
assert torch.allclose(torch.norm(features, dim=1), torch.ones(batch_size))

# 2. æ£€æŸ¥ç›¸ä¼¼åº¦èŒƒå›´
similarities = torch.matmul(features, features.t())
assert similarities.min() >= -1.0 and similarities.max() <= 1.0

# 3. æ£€æŸ¥æŸå¤±å€¼
# åˆå§‹æŸå¤±åº”è¯¥æ¥è¿‘ log(N)ï¼Œå…¶ä¸­ N æ˜¯è´Ÿæ ·æœ¬æ•°
expected_initial_loss = math.log(batch_size - 1)
print(f"Expected initial loss: {expected_initial_loss:.4f}")
```

---

## å…«ã€å®Œæ•´è®­ç»ƒç¤ºä¾‹

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
import math


class ContrastiveModel(nn.Module):
    """ç®€å•çš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹"""
    def __init__(self, input_dim=784, hidden_dim=512, output_dim=128):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x):
        return self.encoder(x)


def train_contrastive(model, dataloader, num_epochs=100):
    """è®­ç»ƒå¯¹æ¯”å­¦ä¹ æ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = InfoNCE(temperature=0.07)
    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_epochs, eta_min=1e-6
    )
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        num_batches = 0
        
        for batch_idx, (x1, x2) in enumerate(dataloader):
            x1, x2 = x1.to(device), x2.to(device)
            
            # è·å–ç‰¹å¾
            z1 = model(x1)  # [batch_size, output_dim]
            z2 = model(x2)  # [batch_size, output_dim]
            
            # åˆå¹¶ç‰¹å¾ [2*batch_size, output_dim]
            features = torch.cat([z1, z2], dim=0)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(features)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 100 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}")
        
        avg_loss = total_loss / num_batches
        scheduler.step()
        print(f"Epoch {epoch} completed, Average Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ¨¡å‹
    model = ContrastiveModel(input_dim=784, hidden_dim=512, output_dim=128)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆéœ€è¦è¿”å›ä¸¤ä¸ªå¢å¼ºç‰ˆæœ¬ï¼‰
    # dataloader = ...
    
    # è®­ç»ƒ
    train_contrastive(model, dataloader, num_epochs=100)
```

---

## ä¹ã€æ€»ç»“ä¸å…³é”®è¦ç‚¹

### 9.1 æ ¸å¿ƒè¦ç‚¹

1. **ç†è®ºåŸºç¡€**ï¼šInfoNCE æ˜¯äº’ä¿¡æ¯çš„ä¸‹ç•Œï¼Œæœ€å¤§åŒ–äº’ä¿¡æ¯ç­‰ä»·äºæœ€å°åŒ– InfoNCE
2. **æ¸©åº¦å‚æ•°**ï¼šæ§åˆ¶åˆ†å¸ƒå°–é”åº¦ï¼Œå°æ¸©åº¦å…³æ³¨å›°éš¾è´Ÿæ ·æœ¬
3. **è´Ÿæ ·æœ¬ç­–ç•¥**ï¼šæ‰¹æ¬¡å†…è´Ÿæ ·æœ¬æœ€å¸¸ç”¨ï¼ŒMoCo ä½¿ç”¨é˜Ÿåˆ—æœºåˆ¶
4. **æ•°å€¼ç¨³å®šæ€§**ï¼šå¿…é¡»è¿›è¡Œ L2 å½’ä¸€åŒ–ï¼Œä½¿ç”¨ log-sum-exp æŠ€å·§

### 9.2 é¢è¯•å›ç­”æ¨¡æ¿

> InfoNCE æ˜¯å¯¹æ¯”å­¦ä¹ çš„æ ¸å¿ƒæŸå¤±å‡½æ•°ï¼Œé€šè¿‡æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹çš„äº’ä¿¡æ¯ã€æœ€å°åŒ–è´Ÿæ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦æ¥å­¦ä¹ è¡¨ç¤ºã€‚å…³é”®ç‚¹åŒ…æ‹¬ï¼šâ‘ æ¸©åº¦å‚æ•°æ§åˆ¶åˆ†å¸ƒå°–é”åº¦ï¼Œé€šå¸¸è®¾ä¸º 0.07ï¼›â‘¡è´Ÿæ ·æœ¬æ•°é‡å½±å“äº’ä¿¡æ¯ä¸‹ç•Œçš„ç´§åº¦ï¼›â‘¢ä¸äº¤å‰ç†µæŸå¤±ç­‰ä»·ï¼Œä½†é€šè¿‡æ¸©åº¦å‚æ•°å®ç°æ›´ç²¾ç»†çš„æ§åˆ¶ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦å¹³è¡¡æ‰¹æ¬¡å¤§å°ã€è´Ÿæ ·æœ¬æ•°é‡å’Œè®¡ç®—æˆæœ¬ã€‚

### 9.3 è¿›ä¸€æ­¥å­¦ä¹ 

- **è®ºæ–‡**ï¼š
  - SimCLR: A Simple Framework for Contrastive Learning
  - MoCo: Momentum Contrast for Unsupervised Visual Representation Learning
  - CLIP: Learning Transferable Visual Models from Natural Language Supervision
  
- **ä»£ç åº“**ï¼š
  - SimCLR: https://github.com/google-research/simclr
  - MoCo: https://github.com/facebookresearch/moco

---
