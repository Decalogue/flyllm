# Tokenizer å®Œå…¨æŒ‡å—

## ä¸€ã€å‰ç½®çŸ¥è¯†

### 1. ä»€ä¹ˆæ˜¯Tokenizerï¼Ÿ

Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰æ˜¯NLPå’ŒLLMä¸­çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—åºåˆ—ã€‚

**æ ¸å¿ƒä½œç”¨ï¼š**
- å°†æ–‡æœ¬åˆ†å‰²æˆæ›´å°çš„å•å…ƒï¼ˆtokensï¼‰
- å°†tokensæ˜ å°„ä¸ºæ•°å­—ID
- å¤„ç†æœªçŸ¥è¯ï¼ˆOOV, Out-of-Vocabularyï¼‰

**ç¤ºä¾‹ï¼š**
```
è¾“å…¥æ–‡æœ¬: "I love AI"
Tokenizerå¤„ç†: ["I", "love", "AI"]
è½¬æ¢ä¸ºID: [101, 2293, 3958]
```

### 2. ä¸ºä»€ä¹ˆéœ€è¦Tokenizerï¼Ÿ

**é—®é¢˜1ï¼šè¯æ±‡è¡¨çˆ†ç‚¸**
- å¦‚æœæŠŠæ¯ä¸ªè¯éƒ½ä½œä¸ºä¸€ä¸ªtokenï¼Œè¯æ±‡è¡¨ä¼šéå¸¸å¤§
- è‹±è¯­æœ‰17ä¸‡+ä¸ªè¯ï¼Œä¸­æ–‡æ›´å¤š
- å¤§è¯æ±‡è¡¨ â†’ æ¨¡å‹å‚æ•°å¤š â†’ è®­ç»ƒæ…¢

**é—®é¢˜2ï¼šæœªçŸ¥è¯ï¼ˆOOVï¼‰**
- è®­ç»ƒæ—¶æ²¡è§è¿‡çš„è¯æ— æ³•å¤„ç†
- ä¾‹å¦‚ï¼šæ–°è¯ã€æ‹¼å†™é”™è¯¯ã€ç½•è§è¯

**é—®é¢˜3ï¼šè¯å½¢å˜åŒ–**
- "run", "running", "runs" æ˜¯åŒä¸€ä¸ªè¯çš„ä¸åŒå½¢å¼
- å¦‚æœéƒ½å½“ä½œç‹¬ç«‹tokenï¼Œæµªè´¹èµ„æº

**è§£å†³æ–¹æ¡ˆï¼šå­è¯åˆ†è¯ï¼ˆSubword Tokenizationï¼‰**
- å°†è¯åˆ†è§£ä¸ºæ›´å°çš„å•å…ƒ
- å¹³è¡¡è¯æ±‡è¡¨å¤§å°å’Œè¡¨è¾¾èƒ½åŠ›
- å¤„ç†OOVé—®é¢˜

### 3. Tokenizerçš„æ¼”è¿›å†å²

**é˜¶æ®µ1ï¼šåŸºäºè¯çš„åˆ†è¯ï¼ˆWord-basedï¼‰**
```python
# ç®€å•çš„æŒ‰ç©ºæ ¼åˆ†è¯
text = "I love AI"
tokens = text.split()  # ['I', 'love', 'AI']
```
**ç¼ºç‚¹ï¼š** è¯æ±‡è¡¨å¤§ã€OOVé—®é¢˜ä¸¥é‡

**é˜¶æ®µ2ï¼šåŸºäºå­—ç¬¦çš„åˆ†è¯ï¼ˆCharacter-basedï¼‰**
```python
# æŒ‰å­—ç¬¦åˆ†è¯
text = "love"
tokens = list(text)  # ['l', 'o', 'v', 'e']
```
**ç¼ºç‚¹ï¼š** åºåˆ—å¤ªé•¿ã€ä¸¢å¤±è¯ä¹‰ä¿¡æ¯

**é˜¶æ®µ3ï¼šå­è¯åˆ†è¯ï¼ˆSubword-basedï¼‰**
```python
# BPEã€WordPieceã€Unigramç­‰
text = "tokenization"
tokens = ["token", "##ization"]  # å¹³è¡¡è¯æ±‡è¡¨å’Œè¡¨è¾¾èƒ½åŠ›
```
**ä¼˜ç‚¹ï¼š** å¹³è¡¡äº†è¯æ±‡è¡¨å¤§å°å’Œè¡¨è¾¾èƒ½åŠ›

### 4. ä¸»æµTokenizerç®—æ³•å¯¹æ¯”

| ç®—æ³• | ä»£è¡¨æ¨¡å‹ | æ ¸å¿ƒæ€æƒ³ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|----------|----------|------|------|
| **BPE** | GPTç³»åˆ— | å­—èŠ‚å¯¹ç¼–ç ï¼Œé¢‘ç¹åˆå¹¶ | ç®€å•é«˜æ•ˆ | è´ªå¿ƒç®—æ³• |
| **WordPiece** | BERT | æœ€å¤§ä¼¼ç„¶ä¼°è®¡ | è€ƒè™‘æ¦‚ç‡ | è®­ç»ƒæ…¢ |
| **Unigram** | T5 | æ¦‚ç‡æ¨¡å‹ | ç†è®ºå®Œå–„ | å®ç°å¤æ‚ |
| **SentencePiece** | LLaMA | ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ | è¯­è¨€æ— å…³ | è¾“å‡ºä¸ä¿ç•™ç©ºæ ¼/è¯ç•Œï¼Œåˆ†è¯ç»“æœä¸ç›´è§‚ |

---

## äºŒã€BPEç®—æ³•è¯¦è§£

### 1. BPEæ ¸å¿ƒæ€æƒ³

**Byte Pair Encodingï¼ˆå­—èŠ‚å¯¹ç¼–ç ï¼‰**
- æœ€åˆç”¨äºæ•°æ®å‹ç¼©
- 2016å¹´å¼•å…¥NLPé¢†åŸŸ
- **æ ¸å¿ƒï¼šè¿­ä»£åˆå¹¶æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹**

### 2. BPEç®—æ³•æµç¨‹

#### è®­ç»ƒé˜¶æ®µ

**è¾“å…¥ï¼š** è¯­æ–™åº“
**è¾“å‡ºï¼š** è¯æ±‡è¡¨ + åˆå¹¶è§„åˆ™

**æ­¥éª¤ï¼š**

```python
# åˆå§‹çŠ¶æ€
è¯æ±‡è¡¨ = æ‰€æœ‰å­—ç¬¦
è¯­æ–™åº“ = ["low", "lower", "newest", "widest"]

# åˆå§‹åŒ–ä¸ºå­—ç¬¦çº§åˆ«
åˆå§‹tokens = {
    "l o w </w>": 5,
    "l o w e r </w>": 2,
    "n e w e s t </w>": 6,
    "w i d e s t </w>": 3
}
# </w>è¡¨ç¤ºè¯å°¾

# è¿­ä»£1: ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡
æœ€é¢‘ç¹çš„å¯¹ = ("e", "s") å‡ºç°9æ¬¡
åˆå¹¶ â†’ "es"
æ›´æ–°è¯æ±‡è¡¨ += ["es"]

# è¿­ä»£2: ç»§ç»­ç»Ÿè®¡
æœ€é¢‘ç¹çš„å¯¹ = ("es", "t") å‡ºç°9æ¬¡
åˆå¹¶ â†’ "est"
æ›´æ–°è¯æ±‡è¡¨ += ["est"]

# è¿­ä»£3...
# é‡å¤ç›´åˆ°è¾¾åˆ°é¢„è®¾è¯æ±‡è¡¨å¤§å°
```

#### æ¨ç†é˜¶æ®µ

**è¾“å…¥ï¼š** æ–°æ–‡æœ¬
**è¾“å‡ºï¼š** Tokenåºåˆ—

**æ­¥éª¤ï¼š**
```python
# 1. åˆå§‹åŒ–ä¸ºå­—ç¬¦åºåˆ—
è¾“å…¥ = "lowest"
åˆå§‹ = ["l", "o", "w", "e", "s", "t"]

# 2. åº”ç”¨å­¦åˆ°çš„åˆå¹¶è§„åˆ™ï¼ˆæŒ‰è®­ç»ƒé¡ºåºï¼‰
è§„åˆ™1: "e" + "s" â†’ "es"
ç»“æœ = ["l", "o", "w", "es", "t"]

è§„åˆ™2: "es" + "t" â†’ "est"
ç»“æœ = ["l", "o", "w", "est"]

è§„åˆ™3: "l" + "o" â†’ "lo"
ç»“æœ = ["lo", "w", "est"]

è§„åˆ™4: "lo" + "w" â†’ "low"
ç»“æœ = ["low", "est"]

# æœ€ç»ˆè¾“å‡º
tokens = ["low", "est"]
```

### 3. BPEä»£ç å®ç°

```python
import re
from collections import defaultdict, Counter

class BPETokenizer:
    def __init__(self, num_merges=1000):
        self.num_merges = num_merges
        self.vocab = set()
        self.merges = {}
        
    def get_stats(self, vocab):
        """ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»tokenå¯¹çš„é¢‘ç‡"""
        pairs = defaultdict(int)
        for word, freq in vocab.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[symbols[i], symbols[i + 1]] += freq
        return pairs
    
    def merge_vocab(self, pair, vocab):
        """åˆå¹¶è¯æ±‡è¡¨ä¸­çš„tokenå¯¹"""
        vocab_out = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)
        
        for word in vocab:
            w_out = word.replace(bigram, replacement)
            vocab_out[w_out] = vocab[word]
        return vocab_out
    
    def train(self, corpus):
        """è®­ç»ƒBPEæ¨¡å‹"""
        # 1. åˆå§‹åŒ–è¯æ±‡è¡¨ï¼ˆå­—ç¬¦çº§åˆ«ï¼‰
        vocab = defaultdict(int)
        for word in corpus:
            # åœ¨è¯å°¾æ·»åŠ </w>æ ‡è®°
            word = ' '.join(list(word)) + ' </w>'
            vocab[word] += 1
        
        # 2. è¿­ä»£åˆå¹¶æœ€é¢‘ç¹çš„tokenå¯¹
        for i in range(self.num_merges):
            pairs = self.get_stats(vocab)
            if not pairs:
                break
            
            # æ‰¾å‡ºé¢‘ç‡æœ€é«˜çš„tokenå¯¹
            best_pair = max(pairs, key=pairs.get)
            vocab = self.merge_vocab(best_pair, vocab)
            self.merges[best_pair] = i
            
            print(f"åˆå¹¶ {i+1}: {best_pair} -> {''.join(best_pair)}")
        
        # 3. æ„å»ºæœ€ç»ˆè¯æ±‡è¡¨
        self.vocab = set()
        for word in vocab:
            self.vocab.update(word.split())
    
    def tokenize(self, text):
        """å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯"""
        # 1. åˆå§‹åŒ–ä¸ºå­—ç¬¦åºåˆ—
        word = ' '.join(list(text)) + ' </w>'
        
        # 2. åº”ç”¨å­¦åˆ°çš„åˆå¹¶è§„åˆ™
        while True:
            pairs = [(word[i:i+2], i) for i in range(len(word.split())-1)]
            if not pairs:
                break
            
            # æ‰¾å‡ºå¯ä»¥åˆå¹¶çš„tokenå¯¹ï¼ˆæŒ‰è®­ç»ƒé¡ºåºï¼‰
            bigram = min(pairs, key=lambda x: self.merges.get(
                tuple(word.split()[x[1]:x[1]+2]), float('inf')
            ))
            
            if tuple(word.split()[bigram[1]:bigram[1]+2]) not in self.merges:
                break
            
            first, second = word.split()[bigram[1]:bigram[1]+2]
            word = word.replace(f"{first} {second}", f"{first}{second}")
        
        return word.split()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è®­ç»ƒè¯­æ–™
    corpus = ["low", "low", "low", "low", "low",
              "lower", "lower",
              "newest", "newest", "newest", "newest", "newest", "newest",
              "widest", "widest", "widest"]
    
    # è®­ç»ƒBPE
    tokenizer = BPETokenizer(num_merges=10)
    tokenizer.train(corpus)
    
    # æµ‹è¯•åˆ†è¯
    print("\nåˆ†è¯ç»“æœ:")
    print(tokenizer.tokenize("lowest"))
    print(tokenizer.tokenize("newest"))
```

### 4. BPEçš„ä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹ï¼š**
- âœ… ç®€å•é«˜æ•ˆï¼Œæ˜“äºå®ç°
- âœ… èƒ½å¤„ç†OOVé—®é¢˜
- âœ… å¹³è¡¡è¯æ±‡è¡¨å¤§å°å’Œè¡¨è¾¾èƒ½åŠ›
- âœ… é€‚ç”¨äºå¤šç§è¯­è¨€

**ç¼ºç‚¹ï¼š**
- âŒ è´ªå¿ƒç®—æ³•ï¼Œå¯èƒ½ä¸æ˜¯æœ€ä¼˜è§£
- âŒ ä¾èµ–äºè®­ç»ƒè¯­æ–™çš„è´¨é‡
- âŒ å¯¹ä¸åŒè¯­è¨€éœ€è¦ä¸åŒå¤„ç†

### 5. BPEçš„å˜ä½“

**Byte-Level BPEï¼ˆGPT-2ï¼‰ï¼š**
- ç›´æ¥åœ¨å­—èŠ‚çº§åˆ«æ“ä½œ
- é¿å…Unicodeå­—ç¬¦çš„å¤æ‚æ€§
- GPT-2ã€GPT-3ä½¿ç”¨

**SentencePiece BPEï¼š**
- ä¸ä¾èµ–é¢„åˆ†è¯
- ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬
- é€‚ç”¨äºæ‰€æœ‰è¯­è¨€

---

## ä¸‰ã€WordPieceç®—æ³•è¯¦è§£

### 1. WordPieceæ ¸å¿ƒæ€æƒ³

**ä¸BPEçš„åŒºåˆ«ï¼š**
- BPEï¼šé€‰æ‹©**é¢‘ç‡æœ€é«˜**çš„tokenå¯¹åˆå¹¶
- WordPieceï¼šé€‰æ‹©**å¯¹æ•°ä¼¼ç„¶å¢åŠ æœ€å¤§**çš„tokenå¯¹åˆå¹¶

**å…¬å¼ï¼š**
```
score(x, y) = log P(xy) / (log P(x) + log P(y))
```

### 2. WordPieceç®—æ³•æµç¨‹

#### è®­ç»ƒé˜¶æ®µ

```python
# 1. åˆå§‹åŒ–ä¸ºå­—ç¬¦çº§åˆ«
è¯æ±‡è¡¨ = æ‰€æœ‰å­—ç¬¦

# 2. è¿­ä»£åˆå¹¶
while è¯æ±‡è¡¨å¤§å° < ç›®æ ‡å¤§å°:
    # ç»Ÿè®¡æ‰€æœ‰å¯èƒ½çš„tokenå¯¹
    for each pair (x, y):
        # è®¡ç®—åˆå¹¶åçš„å¯¹æ•°ä¼¼ç„¶å¢ç›Š
        score = log P(xy) / (log P(x) + log P(y))
    
    # é€‰æ‹©scoreæœ€å¤§çš„tokenå¯¹åˆå¹¶
    best_pair = argmax(score)
    è¯æ±‡è¡¨ += [best_pair]
```

#### æ¨ç†é˜¶æ®µ

```python
# è´ªå¿ƒæœ€é•¿åŒ¹é…ç®—æ³•
def tokenize(word):
    tokens = []
    start = 0
    
    while start < len(word):
        end = len(word)
        found = False
        
        # ä»æœ€é•¿å¼€å§‹å°è¯•åŒ¹é…
        while start < end:
            substr = word[start:end]
            if start > 0:
                substr = "##" + substr  # éé¦–tokenåŠ ##å‰ç¼€
            
            if substr in vocab:
                tokens.append(substr)
                start = end
                found = True
                break
            end -= 1
        
        if not found:
            return [UNK]  # æœªçŸ¥token
    
    return tokens
```

### 3. æ‰‹æ’• WordpieceTokenizer

```python
def whitespace_tokenize(text):
    """ç®€å•çš„ç©ºæ ¼åˆ†è¯"""
    return text.strip().split()

class WordpieceTokenizer:
    def __init__(self, vocab, unk_token, max_chars=100):
        self.vocab = vocab
        self.unk_token = unk_token
        self.max_chars = max_chars

    def tokenize(self, text):
        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_chars:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                # ä»åå¾€å‰éå†ï¼Œæ‰¾åˆ°æœ€é•¿çš„å­ä¸²
                while start < end:
                    substr = ''.join(chars[start:end])
                    if start > 0:
                        substr = '##' + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                # å¦‚æœæ‰¾ä¸åˆ°å­ä¸²ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸€ä¸ªåçš„token
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end
            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)

        return output_tokens
```

### 4. WordPiece vs BPEå¯¹æ¯”

| ç‰¹æ€§ | BPE | WordPiece |
|------|-----|-----------|
| **é€‰æ‹©æ ‡å‡†** | é¢‘ç‡æœ€é«˜ | å¯¹æ•°ä¼¼ç„¶æœ€å¤§ |
| **ç†è®ºåŸºç¡€** | æ•°æ®å‹ç¼© | è¯­è¨€æ¨¡å‹ |
| **è®­ç»ƒé€Ÿåº¦** | å¿« | æ…¢ |
| **ä»£è¡¨æ¨¡å‹** | GPTç³»åˆ— | BERT |
| **å­è¯æ ‡è®°** | æ— ç‰¹æ®Šæ ‡è®° | ##å‰ç¼€ |

---

## å››ã€é¢è¯•é«˜é¢‘é—®é¢˜

### 1. ä¸ºä»€ä¹ˆéœ€è¦Tokenizerï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹ï¼š**
- è¯æ±‡è¡¨çˆ†ç‚¸é—®é¢˜
- OOVï¼ˆæœªçŸ¥è¯ï¼‰é—®é¢˜
- è¯å½¢å˜åŒ–é—®é¢˜
- å­è¯åˆ†è¯æ˜¯æœ€ä½³å¹³è¡¡æ–¹æ¡ˆ

### 2. BPEå’ŒWordPieceçš„åŒºåˆ«ï¼Ÿ

**ç­”æ¡ˆï¼š**
- **BPE**ï¼šé€‰æ‹©é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹åˆå¹¶ï¼ˆè´ªå¿ƒç®—æ³•ï¼‰
- **WordPiece**ï¼šé€‰æ‹©å¯¹æ•°ä¼¼ç„¶å¢åŠ æœ€å¤§çš„å­—ç¬¦å¯¹åˆå¹¶ï¼ˆåŸºäºæ¦‚ç‡ï¼‰
- **å®é™…æ•ˆæœ**ï¼šå·®å¼‚ä¸å¤§ï¼Œä½†WordPieceç†è®ºä¸Šæ›´ä¼˜

### 3. å¦‚ä½•å¤„ç†æœªçŸ¥è¯ï¼ˆOOVï¼‰ï¼Ÿ

**ç­”æ¡ˆï¼š**
- å­è¯åˆ†è¯å¯ä»¥å°†æœªçŸ¥è¯åˆ†è§£ä¸ºå·²çŸ¥å­è¯
- æœ€åæƒ…å†µä¸‹åˆ†è§£ä¸ºå­—ç¬¦çº§åˆ«
- ä¾‹å¦‚ï¼š"tokenization" â†’ ["token", "##ization"]

### 4. Tokenizerçš„è¯æ±‡è¡¨å¤§å°å¦‚ä½•é€‰æ‹©ï¼Ÿ

**ç­”æ¡ˆï¼š**
- å¤ªå°ï¼šåºåˆ—å¤ªé•¿ï¼Œè®­ç»ƒæ…¢
- å¤ªå¤§ï¼šæ¨¡å‹å‚æ•°å¤šï¼Œå†…å­˜æ¶ˆè€—å¤§
- ç»éªŒå€¼ï¼š
  - è‹±æ–‡ï¼š30k-50k
  - ä¸­æ–‡ï¼š20k-40k
  - å¤šè¯­è¨€ï¼š100k+

### 5. ##å‰ç¼€çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

**ç­”æ¡ˆï¼š**
- æ ‡è®°å­è¯çš„è¾¹ç•Œ
- åŒºåˆ†è¯é¦–å’Œéè¯é¦–
- ä¾‹å¦‚ï¼š["token", "##ization"] â†’ "tokenization"

### 6. å¦‚ä½•å¤„ç†ä¸­æ–‡åˆ†è¯ï¼Ÿ

**ç­”æ¡ˆï¼š**
- æ–¹æ³•1ï¼šé¢„åˆ†è¯ï¼ˆjiebaç­‰ï¼‰ + BPE/WordPiece
- æ–¹æ³•2ï¼šå­—ç¬¦çº§åˆ« + BPEï¼ˆé€‚åˆå¤§æ¨¡å‹ï¼‰
- æ–¹æ³•3ï¼šSentencePieceï¼ˆç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ï¼‰

### 7. ä¸ºä»€ä¹ˆGPTä½¿ç”¨BPEï¼ŒBERTä½¿ç”¨WordPieceï¼Ÿ

**ç­”æ¡ˆï¼š**
- **å†å²åŸå› **ï¼šBERTå€Ÿé‰´äº†Googleçš„ç»éªŒ
- **å®é™…æ•ˆæœ**ï¼šä¸¤è€…å·®å¼‚ä¸å¤§
- **å®ç°éš¾åº¦**ï¼šBPEæ›´ç®€å•
- **ç†è®ºåŸºç¡€**ï¼šWordPieceæ›´ä¸¥è°¨

### 8. Tokenizerå¦‚ä½•å½±å“æ¨¡å‹æ€§èƒ½ï¼Ÿ

**ç­”æ¡ˆï¼š**
- è¯æ±‡è¡¨å¤§å°å½±å“æ¨¡å‹å‚æ•°é‡
- åˆ†è¯ç²’åº¦å½±å“åºåˆ—é•¿åº¦
- åˆ†è¯è´¨é‡å½±å“æ¨¡å‹ç†è§£èƒ½åŠ›
- éœ€è¦åœ¨æ•ˆç‡å’Œæ•ˆæœä¹‹é—´å¹³è¡¡

---

## äº”ã€å®æˆ˜æŠ€å·§

### 1. ä½¿ç”¨HuggingFace Tokenizers

```python
from transformers import BertTokenizer, GPT2Tokenizer

# BERTçš„WordPiece
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokens = bert_tokenizer.tokenize("tokenization")
print(tokens)  # ['token', '##ization']

# GPT-2çš„BPE
gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokens = gpt2_tokenizer.tokenize("tokenization")
print(tokens)  # ['token', 'ization']
```

### 2. è®­ç»ƒè‡ªå®šä¹‰Tokenizer

```python
from tokenizers import Tokenizer, models, trainers, pre_tokenizers

# åˆ›å»ºBPEæ¨¡å‹
tokenizer = Tokenizer(models.BPE())

# è®¾ç½®é¢„åˆ†è¯å™¨
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

# è®¾ç½®è®­ç»ƒå™¨
trainer = trainers.BpeTrainer(vocab_size=30000, special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

# è®­ç»ƒ
files = ["corpus.txt"]
tokenizer.train(files, trainer)

# ä¿å­˜
tokenizer.save("my-tokenizer.json")
```

### 3. Tokenizeræ€§èƒ½ä¼˜åŒ–

```python
# 1. æ‰¹é‡å¤„ç†
texts = ["text1", "text2", "text3"]
tokens = tokenizer(texts, padding=True, truncation=True)

# 2. ä½¿ç”¨fast tokenizer
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", use_fast=True)

# 3. å¤šè¿›ç¨‹å¤„ç†
from multiprocessing import Pool
with Pool(4) as p:
    results = p.map(tokenizer.tokenize, texts)
```

---

## å…­ã€æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Tokenizeræ˜¯NLPçš„åŸºç¡€**
   - å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„æ•°å­—
   - å¹³è¡¡è¯æ±‡è¡¨å¤§å°å’Œè¡¨è¾¾èƒ½åŠ›

2. **å­è¯åˆ†è¯æ˜¯ä¸»æµæ–¹æ¡ˆ**
   - BPEï¼šé¢‘ç‡å¯¼å‘ï¼Œç®€å•é«˜æ•ˆ
   - WordPieceï¼šæ¦‚ç‡å¯¼å‘ï¼Œç†è®ºå®Œå–„
   - Unigramï¼šæ¦‚ç‡æ¨¡å‹ï¼Œçµæ´»å¼ºå¤§

3. **é¢è¯•é‡ç‚¹**
   - ç†è§£BPEå’ŒWordPieceçš„åŸç†å’ŒåŒºåˆ«
   - èƒ½æ‰‹å†™åŸºæœ¬çš„Tokenizerä»£ç 
   - äº†è§£OOVå¤„ç†å’Œè¯æ±‡è¡¨å¤§å°é€‰æ‹©

### å­¦ä¹ è·¯å¾„

```
1. ç†è§£åŸºæœ¬æ¦‚å¿µ â†’ ä¸ºä»€ä¹ˆéœ€è¦Tokenizer
2. å­¦ä¹ BPEç®—æ³• â†’ æ ¸å¿ƒåŸç†å’Œä»£ç å®ç°
3. å­¦ä¹ WordPiece â†’ ä¸BPEçš„åŒºåˆ«
4. å®è·µåº”ç”¨ â†’ ä½¿ç”¨HuggingFace Tokenizers
5. æ·±å…¥ç ”ç©¶ â†’ Unigramã€SentencePieceç­‰
```

### æ¨èèµ„æº

**ä»£ç åº“ï¼š**
- HuggingFace Tokenizers: https://github.com/huggingface/tokenizers
- SentencePiece: https://github.com/google/sentencepiece

---

## å…³æ³¨æˆ‘ï¼ŒAIä¸å†éš¾ ğŸš€