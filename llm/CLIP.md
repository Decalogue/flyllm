# CLIPï¼šè§†è§‰-è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹è¯¦è§£ä¸å®ç°

## ğŸ“Œ é¢è¯•æ ¸å¿ƒå›ç­”æ¡†æ¶

### ğŸ’¡ 30ç§’å¿«é€Ÿå›ç­”

> **æ ¸å¿ƒè§‚ç‚¹ï¼š** CLIP (Contrastive Language-Image Pre-training) é€šè¿‡**å¯¹ç§° InfoNCE æŸå¤±**å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬çš„è”åˆè¡¨ç¤ºï¼Œå®ç°é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ã€å›¾æ–‡æ£€ç´¢ç­‰ä»»åŠ¡ã€‚å…³é”®åˆ›æ–°ï¼š**â‘ 4äº¿å›¾æ–‡å¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒ â‘¡å¯¹ç§°å¯¹æ¯”å­¦ä¹ æŸå¤± â‘¢å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°**ï¼Œå¼€åˆ›äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹æ—¶ä»£ã€‚

---

## ä¸€ã€CLIP æ ¸å¿ƒåŸç†

### 1.1 ä»€ä¹ˆæ˜¯ CLIPï¼Ÿ

CLIP (Contrastive Language-Image Pre-training) æ˜¯ OpenAI åœ¨ 2021 å¹´æå‡ºçš„è§†è§‰-è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹ï¼š

**æ ¸å¿ƒæ€æƒ³ï¼š**
- å°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€ä¸ªè¡¨ç¤ºç©ºé—´
- é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰å¯¹é½å›¾åƒå’Œæ–‡æœ¬
- å®ç°é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰å›¾åƒåˆ†ç±»å’Œå›¾æ–‡æ£€ç´¢

**å…³é”®åˆ›æ–°ï¼š**
1. **å¤§è§„æ¨¡æ•°æ®**ï¼š4 äº¿å›¾æ–‡å¯¹ï¼ˆWebImageTextï¼‰
2. **ç®€å•æ¶æ„**ï¼šåŒå¡”ç»“æ„ï¼ˆå›¾åƒç¼–ç å™¨ + æ–‡æœ¬ç¼–ç å™¨ï¼‰
3. **å¯¹æ¯”å­¦ä¹ **ï¼šå¯¹ç§° InfoNCE æŸå¤±
4. **é›¶æ ·æœ¬èƒ½åŠ›**ï¼šæ— éœ€å¾®è°ƒå³å¯ç”¨äºä¸‹æ¸¸ä»»åŠ¡

### 1.2 CLIP æ¶æ„

```
è¾“å…¥ï¼š
  - å›¾åƒï¼šI = [Iâ‚, Iâ‚‚, ..., I_N]  [batch_size, 3, H, W]
  - æ–‡æœ¬ï¼šT = [Tâ‚, Tâ‚‚, ..., T_N]  [batch_size, seq_len]

ç¼–ç å™¨ï¼š
  - å›¾åƒç¼–ç å™¨ï¼šE_image(I) â†’ [batch_size, d]
  - æ–‡æœ¬ç¼–ç å™¨ï¼šE_text(T) â†’ [batch_size, d]

è¾“å‡ºï¼š
  - å›¾åƒç‰¹å¾ï¼šI_emb = normalize(E_image(I))  [batch_size, d]
  - æ–‡æœ¬ç‰¹å¾ï¼šT_emb = normalize(E_text(T))  [batch_size, d]

ç›¸ä¼¼åº¦çŸ©é˜µï¼š
  - logits = Ï„ Â· I_emb @ T_emb^T  [batch_size, batch_size]
  - å¯¹è§’çº¿å…ƒç´ æ˜¯æ­£æ ·æœ¬å¯¹ï¼Œå…¶ä»–æ˜¯è´Ÿæ ·æœ¬å¯¹
```

### 1.3 CLIP vs ä¼ ç»Ÿæ–¹æ³•

| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³• | CLIP |
|------|---------|------|
| **æ•°æ®** | æ ‡æ³¨æ•°æ®é›†ï¼ˆImageNetï¼‰ | ç½‘ç»œçˆ¬å–çš„å›¾æ–‡å¯¹ |
| **è®­ç»ƒ** | ç›‘ç£å­¦ä¹ ï¼ˆåˆ†ç±»ï¼‰ | å¯¹æ¯”å­¦ä¹ ï¼ˆå¯¹é½ï¼‰ |
| **ä»»åŠ¡** | å•ä¸€ä»»åŠ¡ï¼ˆåˆ†ç±»ï¼‰ | å¤šä»»åŠ¡ï¼ˆåˆ†ç±»ã€æ£€ç´¢ã€ç”Ÿæˆï¼‰ |
| **æ³›åŒ–** | éœ€è¦å¾®è°ƒ | é›¶æ ·æœ¬ |
| **è§„æ¨¡** | ç™¾ä¸‡çº§æ ·æœ¬ | 4 äº¿æ ·æœ¬ |

---

## äºŒã€CLIP æŸå¤±å‡½æ•°ï¼šå¯¹ç§° InfoNCE

### 2.1 æŸå¤±å‡½æ•°å…¬å¼

CLIP ä½¿ç”¨**å¯¹ç§° InfoNCE æŸå¤±**ï¼š

```
L_CLIP = (L_imageâ†’text + L_textâ†’image) / 2

å…¶ä¸­ï¼š
L_imageâ†’text = -1/N Î£ log(exp(Ï„ Â· sim(I_i, T_i)) / Î£_j exp(Ï„ Â· sim(I_i, T_j)))
L_textâ†’image = -1/N Î£ log(exp(Ï„ Â· sim(T_i, I_i)) / Î£_j exp(Ï„ Â· sim(T_i, I_j)))
```

**ç¬¦å·è¯´æ˜ï¼š**
- `I_i`ï¼šç¬¬ i ä¸ªå›¾åƒçš„ç‰¹å¾å‘é‡ [d]
- `T_i`ï¼šç¬¬ i ä¸ªæ–‡æœ¬çš„ç‰¹å¾å‘é‡ [d]
- `sim(I, T) = I^T T`ï¼šä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå½’ä¸€åŒ–åï¼‰
- `Ï„`ï¼šå¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°ï¼ˆlogit_scaleï¼‰
- `N`ï¼šæ‰¹æ¬¡å¤§å°

### 2.2 ä¸ºä»€ä¹ˆä½¿ç”¨å¯¹ç§°æŸå¤±ï¼Ÿ

**å¯¹ç§°æ€§çš„é‡è¦æ€§ï¼š**
1. **åŒå‘å¯¹é½**ï¼šç¡®ä¿å›¾åƒâ†’æ–‡æœ¬å’Œæ–‡æœ¬â†’å›¾åƒéƒ½èƒ½æ­£ç¡®åŒ¹é…
2. **è®­ç»ƒç¨³å®š**ï¼šä¸¤ä¸ªæ–¹å‘çš„æ¢¯åº¦ç›¸äº’å¹³è¡¡
3. **æ€§èƒ½æå‡**ï¼šå®éªŒè¯æ˜å¯¹ç§°æŸå¤±æ¯”å•å‘æŸå¤±æ•ˆæœæ›´å¥½

**ç›´è§‚ç†è§£ï¼š**
```
å•å‘æŸå¤±ï¼ˆä»… L_imageâ†’textï¼‰ï¼š
- å›¾åƒå¯ä»¥æ‰¾åˆ°å¯¹åº”çš„æ–‡æœ¬
- ä½†æ–‡æœ¬å¯èƒ½æ‰¾ä¸åˆ°å¯¹åº”çš„å›¾åƒï¼ˆä¸å¯¹ç§°ï¼‰

å¯¹ç§°æŸå¤±ï¼ˆL_imageâ†’text + L_textâ†’imageï¼‰ï¼š
- å›¾åƒå¯ä»¥æ‰¾åˆ°å¯¹åº”çš„æ–‡æœ¬ âœ…
- æ–‡æœ¬ä¹Ÿå¯ä»¥æ‰¾åˆ°å¯¹åº”çš„å›¾åƒ âœ…
- åŒå‘å¯¹é½ï¼Œæ›´ç¨³å®š
```

### 2.3 å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°

CLIP ä½¿ç”¨**å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°**ï¼ˆlogit_scaleï¼‰ï¼š

```python
# åˆå§‹åŒ–
logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

# ä½¿ç”¨
temperature = logit_scale.exp()  # çº¦ç­‰äº 0.07
logits = temperature * image_features @ text_features.t()
```

**ä¸ºä»€ä¹ˆå¯å­¦ä¹ ï¼Ÿ**
- ä¸åŒæ•°æ®é›†çš„ç›¸ä¼¼åº¦åˆ†å¸ƒä¸åŒ
- è®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜æ¸©åº¦
- åˆå§‹å€¼é€šå¸¸è®¾ä¸º log(1/0.07) â‰ˆ 2.66

**æ•°å­¦åŸç†ï¼š**
- æ¸©åº¦å‚æ•°æ§åˆ¶ softmax åˆ†å¸ƒçš„ç†µ
- å°æ¸©åº¦ï¼šåˆ†å¸ƒå°–é”ï¼Œå…³æ³¨å›°éš¾è´Ÿæ ·æœ¬
- å¤§æ¸©åº¦ï¼šåˆ†å¸ƒå¹³æ»‘ï¼Œæ‰€æœ‰æ ·æœ¬æƒé‡ç›¸ä¼¼
- å¯å­¦ä¹ æ¸©åº¦è®©æ¨¡å‹è‡ªé€‚åº”æ•°æ®åˆ†å¸ƒ

**æ¢¯åº¦åˆ†æï¼š**
```
âˆ‚L/âˆ‚logit_scale = âˆ‚L/âˆ‚Ï„ Â· âˆ‚Ï„/âˆ‚logit_scale
                = (Î£ P_neg Â· sim_neg - P_pos Â· sim_pos) Â· Ï„

å…¶ä¸­ï¼š
- P_pos = exp(Ï„Â·sim_pos) / Î£ exp(Ï„Â·sim)
- P_neg = exp(Ï„Â·sim_neg) / Î£ exp(Ï„Â·sim)

å½“ logit_scale å¢å¤§ï¼ˆæ¸©åº¦å¢å¤§ï¼‰ï¼š
- æ¢¯åº¦å€¾å‘äºå‡å°ï¼ˆåˆ†å¸ƒæ›´å¹³æ»‘ï¼‰
- æ¨¡å‹è‡ªåŠ¨å¹³è¡¡æ¸©åº¦å¤§å°
```

### 2.4 æŸå¤±å‡½æ•°çš„æ¢¯åº¦åˆ†æï¼ˆé¢è¯•é‡ç‚¹ï¼‰

**å¯¹å›¾åƒç‰¹å¾çš„æ¢¯åº¦ï¼š**

```
âˆ‚L_imageâ†’text/âˆ‚I_i = -Ï„/T Â· (T_i - Î£_j P_j Â· T_j)

å…¶ä¸­ï¼š
- P_j = exp(Ï„ Â· sim(I_i, T_j)) / Î£_k exp(Ï„ Â· sim(I_i, T_k))
- T_i æ˜¯æ­£æ ·æœ¬æ–‡æœ¬ç‰¹å¾
- T_j æ˜¯è´Ÿæ ·æœ¬æ–‡æœ¬ç‰¹å¾
```

**ç›´è§‚ç†è§£ï¼š**
- æ¢¯åº¦æ¨åŠ¨å›¾åƒç‰¹å¾**é è¿‘æ­£æ ·æœ¬æ–‡æœ¬**
- æ¢¯åº¦æ¨åŠ¨å›¾åƒç‰¹å¾**è¿œç¦»è´Ÿæ ·æœ¬æ–‡æœ¬**ï¼ˆæŒ‰ softmax æƒé‡ï¼‰
- å›°éš¾è´Ÿæ ·æœ¬ï¼ˆç›¸ä¼¼åº¦é«˜ï¼‰è·å¾—æ›´å¤§æƒé‡

**å¯¹æ–‡æœ¬ç‰¹å¾çš„æ¢¯åº¦ï¼ˆå¯¹ç§°ï¼‰ï¼š**

```
âˆ‚L_textâ†’image/âˆ‚T_i = -Ï„/T Â· (I_i - Î£_j P_j Â· I_j)
```

**å¯¹ç§°æ€§çš„æ•°å­¦ä¿è¯ï¼š**
- ä¸¤ä¸ªæ–¹å‘çš„æ¢¯åº¦ç»“æ„ç›¸åŒ
- ç¡®ä¿åŒå‘å¯¹é½çš„ä¸€è‡´æ€§
- è®­ç»ƒè¿‡ç¨‹æ›´ç¨³å®š

### 2.5 ä¸äº’ä¿¡æ¯çš„å…³ç³»

CLIP æŸå¤±æœ€å¤§åŒ–å›¾åƒå’Œæ–‡æœ¬çš„äº’ä¿¡æ¯ï¼š

```
I(I; T) = H(I) - H(I|T) = H(T) - H(T|I)

CLIP æŸå¤±æ˜¯äº’ä¿¡æ¯çš„ä¸‹ç•Œï¼š
I(I; T) â‰¥ log(N) - L_CLIP

å…¶ä¸­ N æ˜¯æ‰¹æ¬¡å¤§å°
```

**è¯æ˜æ€è·¯ï¼š**
- InfoNCE æ˜¯äº’ä¿¡æ¯çš„ä¸‹ç•Œï¼ˆè§ InfoNCE æ–‡æ¡£ï¼‰
- å¯¹ç§°æŸå¤±å–å¹³å‡ï¼Œä¸‹ç•Œå…³ç³»ä¿æŒä¸å˜
- æ‰¹æ¬¡è¶Šå¤§ï¼Œä¸‹ç•Œè¶Šç´§ï¼ˆtightï¼‰

---

## ä¸‰ã€å®Œæ•´å®ç°

### 3.1 CLIP æŸå¤±å‡½æ•°å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Optional


class CLIPLoss(nn.Module):
    """
    CLIP å¯¹ç§° InfoNCE æŸå¤±å‡½æ•°ï¼ˆç”Ÿäº§çº§å®ç°ï¼‰
    
    Reference:
        - CLIP: Learning Transferable Visual Models from Natural Language Supervision
        - https://arxiv.org/abs/2103.00020
    
    Args:
        logit_scale_init: æ¸©åº¦å‚æ•°çš„åˆå§‹å€¼ï¼ˆlog ç©ºé—´ï¼‰ï¼Œé»˜è®¤ log(1/0.07)
        eps: æ•°å€¼ç¨³å®šæ€§å‚æ•°ï¼Œé˜²æ­¢é™¤é›¶
    """
    def __init__(self, logit_scale_init: float = np.log(1 / 0.07), eps: float = 1e-8):
        super().__init__()
        assert logit_scale_init > 0, "logit_scale_init must be positive"
        # å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°ï¼ˆåœ¨ log ç©ºé—´ï¼Œç¡®ä¿ä¸ºæ­£ï¼‰
        self.logit_scale = nn.Parameter(torch.ones([]) * logit_scale_init)
        self.eps = eps
    
    def forward(
        self, 
        image_features: torch.Tensor, 
        text_features: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è®¡ç®— CLIP å¯¹ç§°æŸå¤±
        
        Args:
            image_features: å›¾åƒç‰¹å¾ [batch_size, dim]ï¼Œå·²å½’ä¸€åŒ–
            text_features: æ–‡æœ¬ç‰¹å¾ [batch_size, dim]ï¼Œå·²å½’ä¸€åŒ–
        
        Returns:
            loss: æ€»æŸå¤±ï¼ˆæ ‡é‡ï¼‰
            logits_per_image: å›¾åƒåˆ°æ–‡æœ¬çš„ logits [batch_size, batch_size]
            logits_per_text: æ–‡æœ¬åˆ°å›¾åƒçš„ logits [batch_size, batch_size]
        """
        device = image_features.device
        batch_size = image_features.size(0)
        
        # è¾“å…¥éªŒè¯
        assert image_features.dim() == 2, f"image_features must be 2D, got {image_features.dim()}D"
        assert text_features.dim() == 2, f"text_features must be 2D, got {text_features.dim()}D"
        assert image_features.size(0) == text_features.size(0), \
            f"Batch size mismatch: image {image_features.size(0)} vs text {text_features.size(0)}"
        assert image_features.size(1) == text_features.size(1), \
            f"Feature dim mismatch: image {image_features.size(1)} vs text {text_features.size(1)}"
        
        # ç¡®ä¿ç‰¹å¾å·²å½’ä¸€åŒ–
        image_features = F.normalize(image_features, p=2, dim=1, eps=self.eps)
        text_features = F.normalize(text_features, p=2, dim=1, eps=self.eps)
        
        # è®¡ç®—æ¸©åº¦å‚æ•°
        logit_scale = self.logit_scale.exp()
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        # logits_per_image[i, j] = sim(I_i, T_j)
        logits_per_image = logit_scale * image_features @ text_features.t()
        logits_per_text = logits_per_image.t()
        
        # æ­£æ ·æœ¬å¯¹åœ¨å¯¹è§’çº¿ä¸Š
        labels = torch.arange(batch_size, device=device, dtype=torch.long)
        
        # è®¡ç®—ä¸¤ä¸ªæ–¹å‘çš„æŸå¤±
        loss_image = F.cross_entropy(logits_per_image, labels)
        loss_text = F.cross_entropy(logits_per_text, labels)
        
        # å¯¹ç§°æŸå¤±
        loss = (loss_image + loss_text) / 2
        
        return loss, logits_per_image, logits_per_text
    
    @property
    def temperature(self) -> float:
        """è·å–å½“å‰æ¸©åº¦å‚æ•°å€¼"""
        return self.logit_scale.exp().item()
    
    def extra_repr(self) -> str:
        return f'logit_scale={self.logit_scale.exp().item():.4f}, temperature={self.temperature:.4f}'
    
    def get_accuracy(self, logits_per_image: torch.Tensor, logits_per_text: torch.Tensor) -> Tuple[float, float]:
        """
        è®¡ç®—å‡†ç¡®ç‡ï¼ˆç”¨äºç›‘æ§è®­ç»ƒï¼‰
        
        Returns:
            image_to_text_acc: å›¾åƒåˆ°æ–‡æœ¬çš„å‡†ç¡®ç‡
            text_to_image_acc: æ–‡æœ¬åˆ°å›¾åƒçš„å‡†ç¡®ç‡
        """
        batch_size = logits_per_image.size(0)
        labels = torch.arange(batch_size, device=logits_per_image.device)
        
        image_to_text_acc = (logits_per_image.argmax(dim=1) == labels).float().mean().item()
        text_to_image_acc = (logits_per_text.argmax(dim=1) == labels).float().mean().item()
        
        return image_to_text_acc, text_to_image_acc
```

### 3.2 ç®€åŒ–ç‰ˆæœ¬ï¼ˆå›ºå®šæ¸©åº¦ï¼‰

```python
class CLIPLossFixedTemp(nn.Module):
    """
    CLIP æŸå¤±å‡½æ•°ï¼ˆå›ºå®šæ¸©åº¦ç‰ˆæœ¬ï¼‰
    é€‚ç”¨äºä¸éœ€è¦å­¦ä¹ æ¸©åº¦çš„åœºæ™¯
    """
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        assert temperature > 0, "Temperature must be positive"
        self.temperature = temperature
    
    def forward(
        self, 
        image_features: torch.Tensor, 
        text_features: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            image_features: [batch_size, dim]ï¼Œå·²å½’ä¸€åŒ–
            text_features: [batch_size, dim]ï¼Œå·²å½’ä¸€åŒ–
        
        Returns:
            loss: å¯¹ç§° CLIP æŸå¤±
        """
        device = image_features.device
        batch_size = image_features.size(0)
        
        # å½’ä¸€åŒ–
        image_features = F.normalize(image_features, p=2, dim=1, eps=1e-8)
        text_features = F.normalize(text_features, p=2, dim=1, eps=1e-8)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        logits_per_image = (image_features @ text_features.t()) / self.temperature
        logits_per_text = logits_per_image.t()
        
        # æ ‡ç­¾ï¼ˆå¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬ï¼‰
        labels = torch.arange(batch_size, device=device, dtype=torch.long)
        
        # å¯¹ç§°æŸå¤±
        loss_image = F.cross_entropy(logits_per_image, labels)
        loss_text = F.cross_entropy(logits_per_text, labels)
        
        return (loss_image + loss_text) / 2
```

### 3.3 å®Œæ•´ CLIP æ¨¡å‹å®ç°

```python
class CLIP(nn.Module):
    """
    å®Œæ•´çš„ CLIP æ¨¡å‹å®ç°
    
    Args:
        image_encoder: å›¾åƒç¼–ç å™¨ï¼ˆå¦‚ ResNetã€ViTï¼‰
        text_encoder: æ–‡æœ¬ç¼–ç å™¨ï¼ˆå¦‚ Transformerï¼‰
        embed_dim: ç‰¹å¾ç»´åº¦
        logit_scale_init: æ¸©åº¦å‚æ•°åˆå§‹å€¼
    """
    def __init__(
        self,
        image_encoder: nn.Module,
        text_encoder: nn.Module,
        embed_dim: int = 512,
        logit_scale_init: float = np.log(1 / 0.07)
    ):
        super().__init__()
        self.image_encoder = image_encoder
        self.text_encoder = text_encoder
        
        # æŠ•å½±å±‚ï¼šå°†ç¼–ç å™¨è¾“å‡ºæŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        self.image_projection = nn.Linear(image_encoder.output_dim, embed_dim)
        self.text_projection = nn.Linear(text_encoder.output_dim, embed_dim)
        
        # æŸå¤±å‡½æ•°
        self.loss_fn = CLIPLoss(logit_scale_init=logit_scale_init)
    
    def encode_image(self, image: torch.Tensor) -> torch.Tensor:
        """
        ç¼–ç å›¾åƒ
        
        Args:
            image: [batch_size, 3, H, W]
        
        Returns:
            image_features: [batch_size, embed_dim]ï¼Œå·²å½’ä¸€åŒ–
        """
        # ç¼–ç 
        image_features = self.image_encoder(image)  # [batch_size, image_dim]
        
        # æŠ•å½±
        image_features = self.image_projection(image_features)  # [batch_size, embed_dim]
        
        # å½’ä¸€åŒ–
        image_features = F.normalize(image_features, p=2, dim=1, eps=1e-8)
        
        return image_features
    
    def encode_text(self, text: torch.Tensor) -> torch.Tensor:
        """
        ç¼–ç æ–‡æœ¬
        
        Args:
            text: [batch_size, seq_len] æˆ–å­—å…¸æ ¼å¼
        
        Returns:
            text_features: [batch_size, embed_dim]ï¼Œå·²å½’ä¸€åŒ–
        """
        # ç¼–ç 
        text_features = self.text_encoder(text)  # [batch_size, text_dim]
        
        # æŠ•å½±
        text_features = self.text_projection(text_features)  # [batch_size, embed_dim]
        
        # å½’ä¸€åŒ–
        text_features = F.normalize(text_features, p=2, dim=1, eps=1e-8)
        
        return text_features
    
    def forward(
        self, 
        image: torch.Tensor, 
        text: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            image: [batch_size, 3, H, W]
            text: [batch_size, seq_len]
        
        Returns:
            loss: æŸå¤±å€¼
            logits_per_image: [batch_size, batch_size]
            logits_per_text: [batch_size, batch_size]
        """
        # ç¼–ç 
        image_features = self.encode_image(image)
        text_features = self.encode_text(text)
        
        # è®¡ç®—æŸå¤±
        loss, logits_per_image, logits_per_text = self.loss_fn(
            image_features, text_features
        )
        
        return loss, logits_per_image, logits_per_text
```

### 3.4 ä½¿ç”¨ Hugging Face Transformers çš„ç®€åŒ–å®ç°

```python
from transformers import CLIPModel, CLIPProcessor
import torch

class CLIPWrapper:
    """
    CLIP æ¨¡å‹åŒ…è£…ç±»ï¼ˆä½¿ç”¨ Hugging Faceï¼‰
    """
    def __init__(self, model_name: str = "openai/clip-vit-base-patch32"):
        self.model = CLIPModel.from_pretrained(model_name)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()
    
    def encode_image(self, images):
        """ç¼–ç å›¾åƒ"""
        inputs = self.processor(images=images, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model.get_image_features(**inputs)
        return outputs
    
    def encode_text(self, texts):
        """ç¼–ç æ–‡æœ¬"""
        inputs = self.processor(text=texts, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model.get_text_features(**inputs)
        return outputs
    
    def compute_similarity(self, images, texts):
        """è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦"""
        image_features = self.encode_image(images)
        text_features = self.encode_text(texts)
        
        # å½’ä¸€åŒ–
        image_features = F.normalize(image_features, p=2, dim=1)
        text_features = F.normalize(text_features, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = image_features @ text_features.t()
        return similarity
```

---

## å››ã€è®­ç»ƒç»†èŠ‚

### 4.1 æ•°æ®å‡†å¤‡

```python
class CLIPDataset(torch.utils.data.Dataset):
    """
    CLIP æ•°æ®é›†
    æ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€ä¸ªå›¾åƒå’Œä¸€ä¸ªå¯¹åº”çš„æ–‡æœ¬æè¿°
    """
    def __init__(self, image_paths, texts, transform=None, tokenizer=None):
        self.image_paths = image_paths
        self.texts = texts
        self.transform = transform
        self.tokenizer = tokenizer
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        image = Image.open(self.image_paths[idx]).convert('RGB')
        if self.transform:
            image = self.transform(image)
        
        # å¤„ç†æ–‡æœ¬
        text = self.texts[idx]
        if self.tokenizer:
            text = self.tokenizer(
                text, 
                return_tensors="pt", 
                padding="max_length",
                max_length=77,  # CLIP æ–‡æœ¬æœ€å¤§é•¿åº¦
                truncation=True
            )
        
        return image, text
```

### 4.2 è®­ç»ƒå¾ªç¯

```python
def train_clip(
    model: CLIP,
    dataloader: torch.utils.data.DataLoader,
    num_epochs: int = 32,
    learning_rate: float = 5e-4,
    warmup_steps: int = 2000
):
    """
    è®­ç»ƒ CLIP æ¨¡å‹
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.train()
    
    # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨ AdamWï¼‰
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        betas=(0.9, 0.98),
        eps=1e-6,
        weight_decay=0.2
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆcosine decay with warmupï¼‰
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            return 0.5 * (1 + np.cos(np.pi * (step - warmup_steps) / (num_epochs * len(dataloader) - warmup_steps)))
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # è®­ç»ƒå¾ªç¯
    global_step = 0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        num_batches = 0
        
        for batch_idx, (images, texts) in enumerate(dataloader):
            images = images.to(device)
            # texts å¯èƒ½æ˜¯å­—å…¸æ ¼å¼
            if isinstance(texts, dict):
                texts = {k: v.to(device) for k, v in texts.items()}
            else:
                texts = texts.to(device)
            
            # å‰å‘ä¼ æ’­
            loss, logits_per_image, logits_per_text = model(images, texts)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            
            epoch_loss += loss.item()
            num_batches += 1
            global_step += 1
            
            if batch_idx % 100 == 0:
                current_lr = scheduler.get_last_lr()[0]
                print(
                    f"Epoch {epoch}, Batch {batch_idx}, "
                    f"Loss: {loss.item():.4f}, "
                    f"LR: {current_lr:.6f}, "
                    f"Temperature: {model.loss_fn.temperature:.4f}"
                )
        
        avg_loss = epoch_loss / num_batches
        print(f"Epoch {epoch} completed, Average Loss: {avg_loss:.4f}")
```

### 4.3 å…³é”®è¶…å‚æ•°

| è¶…å‚æ•° | å€¼ | è¯´æ˜ |
|--------|-----|------|
| **æ‰¹æ¬¡å¤§å°** | 32,768 | éå¸¸å¤§çš„æ‰¹æ¬¡ï¼ˆä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼‰ |
| **å­¦ä¹ ç‡** | 5e-4 | AdamW ä¼˜åŒ–å™¨ |
| **Warmup** | 2,000 steps | çº¿æ€§ warmup |
| **å­¦ä¹ ç‡è°ƒåº¦** | Cosine decay | ä½™å¼¦é€€ç« |
| **æ¸©åº¦åˆå§‹å€¼** | log(1/0.07) | å¯å­¦ä¹  |
| **æƒé‡è¡°å‡** | 0.2 | L2 æ­£åˆ™åŒ– |
| **æ¢¯åº¦è£å‰ª** | 1.0 | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ |

---

## äº”ã€åº”ç”¨åœºæ™¯

### 5.1 é›¶æ ·æœ¬å›¾åƒåˆ†ç±»

```python
def zero_shot_classification(
    model: CLIP,
    image: torch.Tensor,
    class_names: list,
    device: torch.device
) -> dict:
    """
    é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
    
    Args:
        model: CLIP æ¨¡å‹
        image: è¾“å…¥å›¾åƒ [1, 3, H, W]
        class_names: ç±»åˆ«åç§°åˆ—è¡¨ï¼Œå¦‚ ["cat", "dog", "bird"]
        device: è®¾å¤‡
    
    Returns:
        predictions: é¢„æµ‹ç»“æœå­—å…¸
    """
    model.eval()
    
    # æ„å»ºæ–‡æœ¬æç¤º
    texts = [f"a photo of a {name}" for name in class_names]
    
    # ç¼–ç 
    with torch.no_grad():
        image_features = model.encode_image(image.to(device))
        text_features = model.encode_text(texts)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    logits_per_image = model.loss_fn.logit_scale.exp() * image_features @ text_features.t()
    probs = F.softmax(logits_per_image, dim=1)
    
    # è·å–é¢„æµ‹
    top_probs, top_indices = torch.topk(probs, k=len(class_names))
    
    predictions = {
        class_names[idx]: prob.item()
        for prob, idx in zip(top_probs[0], top_indices[0])
    }
    
    return predictions
```

### 5.2 å›¾æ–‡æ£€ç´¢

```python
def image_text_retrieval(
    model: CLIP,
    query_images: torch.Tensor,
    candidate_texts: list,
    top_k: int = 5,
    device: torch.device = None
) -> list:
    """
    å›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢
    
    Args:
        model: CLIP æ¨¡å‹
        query_images: æŸ¥è¯¢å›¾åƒ [N, 3, H, W]
        candidate_texts: å€™é€‰æ–‡æœ¬åˆ—è¡¨
        top_k: è¿”å› top-k ç»“æœ
        device: è®¾å¤‡
    
    Returns:
        results: æ£€ç´¢ç»“æœåˆ—è¡¨
    """
    if device is None:
        device = next(model.parameters()).device
    
    model.eval()
    
    with torch.no_grad():
        # ç¼–ç 
        image_features = model.encode_image(query_images.to(device))
        text_features = model.encode_text(candidate_texts)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = image_features @ text_features.t()  # [N, M]
        
        # è·å– top-k
        top_similarities, top_indices = torch.topk(similarity, k=top_k, dim=1)
    
    results = []
    for i in range(len(query_images)):
        result = [
            {
                "text": candidate_texts[idx.item()],
                "score": sim.item()
            }
            for sim, idx in zip(top_similarities[i], top_indices[i])
        ]
        results.append(result)
    
    return results
```

### 5.3 æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢

```python
def text_image_retrieval(
    model: CLIP,
    query_text: str,
    candidate_images: torch.Tensor,
    top_k: int = 5,
    device: torch.device = None
) -> list:
    """
    æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢
    
    Args:
        model: CLIP æ¨¡å‹
        query_text: æŸ¥è¯¢æ–‡æœ¬
        candidate_images: å€™é€‰å›¾åƒ [M, 3, H, W]
        top_k: è¿”å› top-k ç»“æœ
        device: è®¾å¤‡
    
    Returns:
        results: æ£€ç´¢ç»“æœï¼ˆå›¾åƒç´¢å¼•å’Œç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
    """
    if device is None:
        device = next(model.parameters()).device
    
    model.eval()
    
    with torch.no_grad():
        # ç¼–ç 
        text_features = model.encode_text([query_text])
        image_features = model.encode_image(candidate_images.to(device))
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = text_features @ image_features.t()  # [1, M]
        
        # è·å– top-k
        top_similarities, top_indices = torch.topk(similarity, k=top_k, dim=1)
    
    results = [
        {
            "image_idx": idx.item(),
            "score": sim.item()
        }
        for sim, idx in zip(top_similarities[0], top_indices[0])
    ]
    
    return results
```

---

## å…­ã€é¢è¯•å¸¸è§é—®é¢˜ï¼ˆQ&Aï¼‰

### Q1: CLIP çš„æŸå¤±å‡½æ•°æ˜¯ InfoNCE å—ï¼Ÿ

**A:** æ˜¯çš„ï¼ŒCLIP ä½¿ç”¨**å¯¹ç§° InfoNCE æŸå¤±**ï¼š

```python
# æ ‡å‡† InfoNCEï¼ˆå•å‘ï¼‰
L = -log(exp(sim(z, z^+) / Ï„) / Î£_j exp(sim(z, z_j) / Ï„))

# CLIP æŸå¤±ï¼ˆå¯¹ç§°ï¼‰
L_CLIP = (L_imageâ†’text + L_textâ†’image) / 2
```

**å…³é”®åŒºåˆ«ï¼š**
- **æ ‡å‡† InfoNCE**ï¼šå•å‘ï¼ˆå¦‚ SimCLR çš„å›¾åƒâ†’å›¾åƒï¼‰
- **CLIP æŸå¤±**ï¼šåŒå‘å¯¹ç§°ï¼ˆå›¾åƒâ†’æ–‡æœ¬ + æ–‡æœ¬â†’å›¾åƒï¼‰
- **æ¸©åº¦å‚æ•°**ï¼šCLIP ä½¿ç”¨å¯å­¦ä¹ çš„ logit_scale

### Q2: ä¸ºä»€ä¹ˆ CLIP éœ€è¦å¯¹ç§°æŸå¤±ï¼Ÿ

**A:** ä¸‰ä¸ªåŸå› ï¼š

1. **åŒå‘å¯¹é½**ï¼šç¡®ä¿å›¾åƒå’Œæ–‡æœ¬éƒ½èƒ½æ­£ç¡®åŒ¹é…å¯¹æ–¹
2. **è®­ç»ƒç¨³å®š**ï¼šä¸¤ä¸ªæ–¹å‘çš„æ¢¯åº¦ç›¸äº’å¹³è¡¡ï¼Œé¿å…å•å‘åå·®
3. **æ€§èƒ½æå‡**ï¼šå®éªŒè¯æ˜å¯¹ç§°æŸå¤±æ¯”å•å‘æŸå¤±æ•ˆæœæ›´å¥½ï¼ˆ+2-3%ï¼‰

**å®éªŒå¯¹æ¯”ï¼š**
| æŸå¤±ç±»å‹ | ImageNet é›¶æ ·æœ¬å‡†ç¡®ç‡ |
|---------|---------------------|
| ä»… L_imageâ†’text | 58.2% |
| ä»… L_textâ†’image | 59.1% |
| å¯¹ç§°æŸå¤± | **61.5%** âœ… |

### Q3: CLIP çš„æ¸©åº¦å‚æ•°ä¸ºä»€ä¹ˆæ˜¯å¯å­¦ä¹ çš„ï¼Ÿ

**A:** 

1. **æ•°æ®åˆ†å¸ƒä¸åŒ**ï¼šä¸åŒæ•°æ®é›†çš„ç›¸ä¼¼åº¦åˆ†å¸ƒå·®å¼‚å¤§
2. **è‡ªé€‚åº”è°ƒæ•´**ï¼šè®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜æ¸©åº¦
3. **æ€§èƒ½æå‡**ï¼šå¯å­¦ä¹ æ¸©åº¦æ¯”å›ºå®šæ¸©åº¦æ•ˆæœæ›´å¥½

**å®ç°ç»†èŠ‚ï¼š**
```python
# åœ¨ log ç©ºé—´åˆå§‹åŒ–ï¼Œç¡®ä¿æ¸©åº¦å§‹ç»ˆä¸ºæ­£
logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
temperature = logit_scale.exp()  # çº¦ç­‰äº 0.07
```

**å…¸å‹å€¼ï¼š**
- åˆå§‹å€¼ï¼šlog(1/0.07) â‰ˆ 2.66
- è®­ç»ƒåï¼šé€šå¸¸æ”¶æ•›åˆ° 2.5-3.0ï¼ˆå¯¹åº”æ¸©åº¦ 0.08-0.05ï¼‰

### Q4: CLIP å’Œ SimCLR çš„åŒºåˆ«ï¼Ÿ

**A:** 

| ç»´åº¦ | SimCLR | CLIP |
|------|--------|------|
| **æ¨¡æ€** | å•æ¨¡æ€ï¼ˆå›¾åƒï¼‰ | å¤šæ¨¡æ€ï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰ |
| **æ­£æ ·æœ¬** | åŒä¸€å›¾åƒçš„ä¸åŒå¢å¼º | é…å¯¹çš„å›¾åƒå’Œæ–‡æœ¬ |
| **æŸå¤±** | å•å‘ InfoNCE | å¯¹ç§° InfoNCE |
| **æ¸©åº¦** | å›ºå®š 0.07 | å¯å­¦ä¹  |
| **åº”ç”¨** | å›¾åƒè¡¨ç¤ºå­¦ä¹  | å›¾æ–‡å¯¹é½ |

**ä»£ç å¯¹æ¯”ï¼š**
```python
# SimCLRï¼šå›¾åƒâ†’å›¾åƒ
features = [z1, z1', z2, z2', ...]  # æ¯ä¸¤ä¸ªæ˜¯æ­£æ ·æœ¬å¯¹
loss = InfoNCE(features)

# CLIPï¼šå›¾åƒâ†”æ–‡æœ¬
image_features = encode_image(images)
text_features = encode_text(texts)
loss = (InfoNCE(imageâ†’text) + InfoNCE(textâ†’image)) / 2
```

### Q5: CLIP å¦‚ä½•å®ç°é›¶æ ·æœ¬åˆ†ç±»ï¼Ÿ

**A:** ä¸‰ä¸ªæ­¥éª¤ï¼š

1. **æ„å»ºæ–‡æœ¬æç¤º**ï¼šå°†ç±»åˆ«åè½¬æ¢ä¸ºæ–‡æœ¬æè¿°
   ```python
   class_names = ["cat", "dog", "bird"]
   texts = [f"a photo of a {name}" for name in class_names]
   ```

2. **ç¼–ç å›¾åƒå’Œæ–‡æœ¬**ï¼š
   ```python
   image_features = model.encode_image(image)
   text_features = model.encode_text(texts)
   ```

3. **è®¡ç®—ç›¸ä¼¼åº¦å¹¶åˆ†ç±»**ï¼š
   ```python
   similarity = image_features @ text_features.t()
   predictions = torch.argmax(similarity, dim=1)
   ```

**ä¼˜åŠ¿ï¼š**
- æ— éœ€è®­ç»ƒåˆ†ç±»å¤´
- å¯ä»¥è½»æ¾æ‰©å±•åˆ°æ–°ç±»åˆ«
- æ”¯æŒè‡ªç„¶è¯­è¨€æè¿°

### Q6: CLIP çš„å±€é™æ€§ï¼Ÿ

**A:** 

1. **æ•°æ®åå·®**ï¼šç½‘ç»œæ•°æ®å­˜åœ¨åè§ï¼ˆæ€§åˆ«ã€ç§æ—ç­‰ï¼‰
2. **ç»†ç²’åº¦ä»»åŠ¡**ï¼šåœ¨ç»†ç²’åº¦åˆ†ç±»ä¸Šè¡¨ç°è¾ƒå·®
3. **è®¡ç®—æˆæœ¬**ï¼šéœ€è¦å¤§è§„æ¨¡é¢„è®­ç»ƒï¼ˆ4 äº¿æ ·æœ¬ï¼‰
4. **æ–‡æœ¬ç†è§£**ï¼šå¯¹å¤æ‚æ–‡æœ¬ç†è§£æœ‰é™
5. **é›¶æ ·æœ¬æ€§èƒ½**ï¼šä»ä½äºæœ‰ç›‘ç£å¾®è°ƒ

### Q7: CLIP vs ALIGN vs BLIP çš„åŒºåˆ«ï¼Ÿ

**A:** 

| ç»´åº¦ | CLIP | ALIGN | BLIP |
|------|------|-------|------|
| **æ•°æ®è§„æ¨¡** | 4 äº¿ | 10 äº¿ | 1.29 äº¿ |
| **æ¶æ„** | åŒå¡” | åŒå¡” | ç¼–ç å™¨-è§£ç å™¨ |
| **æŸå¤±å‡½æ•°** | å¯¹ç§° InfoNCE | å¯¹ç§° InfoNCE | InfoNCE + LM |
| **æ–‡æœ¬ç¼–ç å™¨** | Transformer | Transformer | BERT |
| **å›¾åƒç¼–ç å™¨** | ViT/ResNet | EfficientNet | ViT |
| **ç”Ÿæˆèƒ½åŠ›** | âŒ | âŒ | âœ… |
| **é›¶æ ·æœ¬** | âœ… | âœ… | âŒ |

**å…³é”®åŒºåˆ«ï¼š**
- **ALIGN**ï¼šæ›´å¤§è§„æ¨¡æ•°æ®ï¼Œç±»ä¼¼æ¶æ„
- **BLIP**ï¼šå¼•å…¥ç”Ÿæˆä»»åŠ¡ï¼Œæ”¯æŒå›¾åƒæè¿°ç”Ÿæˆ

### Q8: CLIP çš„æ‰¹æ¬¡å¤§å°ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤§ï¼ˆ32Kï¼‰ï¼Ÿ

**A:** ä¸‰ä¸ªåŸå› ï¼š

1. **è´Ÿæ ·æœ¬æ•°é‡**ï¼šæ‰¹æ¬¡è¶Šå¤§ï¼Œè´Ÿæ ·æœ¬è¶Šå¤šï¼Œäº’ä¿¡æ¯ä¸‹ç•Œè¶Šç´§
2. **è®­ç»ƒç¨³å®š**ï¼šå¤§æ‰¹æ¬¡ä½¿æ¢¯åº¦ä¼°è®¡æ›´å‡†ç¡®
3. **æ€§èƒ½æå‡**ï¼šå®éªŒè¯æ˜å¤§æ‰¹æ¬¡æ˜¾è‘—æå‡æ€§èƒ½

**å®éªŒæ•°æ®ï¼š**
| æ‰¹æ¬¡å¤§å° | ImageNet é›¶æ ·æœ¬å‡†ç¡®ç‡ |
|---------|---------------------|
| 1,024 | 58.1% |
| 4,096 | 60.2% |
| 16,384 | 61.3% |
| 32,768 | **61.5%** âœ… |

**å®ç°æ–¹å¼ï¼š**
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡
- å¤š GPU åˆ†å¸ƒå¼è®­ç»ƒ
- æ··åˆç²¾åº¦è®­ç»ƒèŠ‚çœå†…å­˜

### Q9: CLIP çš„æ–‡æœ¬æç¤ºï¼ˆPromptï¼‰å·¥ç¨‹ï¼Ÿ

**A:** 

**é›¶æ ·æœ¬åˆ†ç±»ä¸­çš„æç¤ºå·¥ç¨‹ï¼š**

```python
# åŸºç¡€æç¤º
"a photo of a {class_name}"

# æ›´å¥½çš„æç¤ºæ¨¡æ¿
templates = [
    "a photo of a {}",
    "a rendering of a {}",
    "a cropped photo of a {}",
    "the photo of a {}",
    "a photo of a clean {}",
    "a photo of a dirty {}",
    "a dark photo of a {}",
    "a photo of my {}",
    "a photo of the cool {}",
    "a close-up photo of a {}",
    "a bright photo of a {}",
    "a cropped photo of a {}",
    "a photo of a {}",
    "a good photo of a {}",
    "a photo of one {}",
    "a close-up photo of a {}",
    "a rendition of a {}",
    "a photo of a clean {}",
    "a rendition of a {}",
    "a photo of a nice {}",
    "a good photo of a {}",
    "a photo of a wonderful {}",
    "a photo of a {}",
    "a photo of a large {}",
    "a photo of a cool {}",
    "a photo of a small {}",
]
```

**ä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ªæç¤ºï¼Ÿ**
- ä¸åŒæç¤ºæ•è·ä¸åŒçš„è§†è§‰ç‰¹å¾
- å¹³å‡å¤šä¸ªæç¤ºçš„ç»“æœæ›´é²æ£’
- æå‡ 1-2% çš„å‡†ç¡®ç‡

### Q10: CLIP å¦‚ä½•æ‰©å±•åˆ°è§†é¢‘ä»»åŠ¡ï¼Ÿ

**A:** 

**æ–¹æ³•1ï¼šå¸§çº§èšåˆ**
```python
# å¯¹è§†é¢‘çš„æ¯ä¸€å¸§ç¼–ç ï¼Œç„¶åå¹³å‡
video_frames = [frame1, frame2, ..., frameN]
frame_features = [model.encode_image(frame) for frame in video_frames]
video_feature = torch.mean(torch.stack(frame_features), dim=0)
```

**æ–¹æ³•2ï¼šæ—¶é—´æ³¨æ„åŠ›**
```python
# ä½¿ç”¨æ—¶é—´æ³¨æ„åŠ›èšåˆå¸§ç‰¹å¾
frame_features = model.encode_image(video_frames)
video_feature = temporal_attention(frame_features)
```

**ç›¸å…³æ–¹æ³•ï¼š**
- **VideoCLIP**ï¼šæ‰©å±• CLIP åˆ°è§†é¢‘
- **CLIP4Clip**ï¼šè§†é¢‘-æ–‡æœ¬æ£€ç´¢
- **X-CLIP**ï¼šè·¨æ¨¡æ€è§†é¢‘ç†è§£

---

## ä¸ƒã€ä¸å…¶ä»–å¤šæ¨¡æ€æ–¹æ³•å¯¹æ¯”

### 7.1 CLIP vs ALIGN vs BLIP vs Flamingo

| æ–¹æ³• | å¹´ä»½ | æ•°æ®è§„æ¨¡ | æ¶æ„ | æŸå¤± | ç‰¹ç‚¹ |
|------|------|---------|------|------|------|
| **CLIP** | 2021 | 4 äº¿ | åŒå¡” | å¯¹ç§° InfoNCE | é›¶æ ·æœ¬èƒ½åŠ›å¼º |
| **ALIGN** | 2021 | 10 äº¿ | åŒå¡” | å¯¹ç§° InfoNCE | æ›´å¤§è§„æ¨¡æ•°æ® |
| **BLIP** | 2022 | 1.29 äº¿ | ç¼–ç å™¨-è§£ç å™¨ | InfoNCE + LM | æ”¯æŒç”Ÿæˆ |
| **Flamingo** | 2022 | å¤§è§„æ¨¡ | å¤šæ¨¡æ€ LLM | äº¤å‰ç†µ | å°‘æ ·æœ¬å­¦ä¹  |

**å…³é”®åŒºåˆ«ï¼š**

1. **CLIP/ALIGN**ï¼šçº¯å¯¹æ¯”å­¦ä¹ ï¼Œé›¶æ ·æœ¬èƒ½åŠ›å¼º
2. **BLIP**ï¼šå¼•å…¥ç”Ÿæˆä»»åŠ¡ï¼Œæ”¯æŒå›¾åƒæè¿°
3. **Flamingo**ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼Œå°‘æ ·æœ¬å­¦ä¹ 

### 7.2 CLIP vs ä¼ ç»Ÿè§†è§‰-è¯­è¨€æ¨¡å‹

| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³• | CLIP |
|------|---------|------|
| **é¢„è®­ç»ƒä»»åŠ¡** | å›¾åƒæè¿°ç”Ÿæˆ | å¯¹æ¯”å­¦ä¹  |
| **æ•°æ®éœ€æ±‚** | é«˜è´¨é‡æ ‡æ³¨ | ç½‘ç»œçˆ¬å– |
| **æ¶æ„** | å¤æ‚ï¼ˆå¤šä»»åŠ¡ï¼‰ | ç®€å•ï¼ˆåŒå¡”ï¼‰ |
| **é›¶æ ·æœ¬** | âŒ | âœ… |
| **æ‰©å±•æ€§** | å›°éš¾ | å®¹æ˜“ |

---

## å…«ã€æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 8.1 æ¢¯åº¦ç´¯ç§¯ï¼ˆå¤§æ‰¹æ¬¡è®­ç»ƒï¼‰

```python
def train_with_gradient_accumulation(
    model, dataloader, accumulation_steps=8
):
    """
    ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡è®­ç»ƒ
    """
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)
    
    for batch_idx, (images, texts) in enumerate(dataloader):
        loss, _, _ = model(images, texts)
        loss = loss / accumulation_steps  # ç¼©æ”¾æŸå¤±
        loss.backward()
        
        if (batch_idx + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
```

### 8.2 æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for images, texts in dataloader:
    with autocast():
        loss, _, _ = model(images, texts)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
```

### 8.3 æ•°æ®å¢å¼º

```python
from torchvision import transforms

# å›¾åƒå¢å¼º
image_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])
```

---

### 8.4 åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤š GPUï¼‰

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    dist.init_process_group(backend='nccl')
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    return rank, world_size

def train_distributed(model, dataloader):
    """åˆ†å¸ƒå¼è®­ç»ƒ"""
    rank, world_size = setup_distributed()
    device = torch.device(f'cuda:{rank}')
    
    model = model.to(device)
    model = DDP(model, device_ids=[rank])
    
    # ä½¿ç”¨ DistributedSampler
    sampler = torch.utils.data.distributed.DistributedSampler(
        dataloader.dataset, num_replicas=world_size, rank=rank
    )
    dataloader = torch.utils.data.DataLoader(
        dataloader.dataset, batch_size=dataloader.batch_size, sampler=sampler
    )
    
    # è®­ç»ƒå¾ªç¯ï¼ˆåŒå• GPUï¼‰
    for images, texts in dataloader:
        # ...
        pass
```

### 8.5 ç‰¹å¾ç¼“å­˜ï¼ˆæ¨ç†ä¼˜åŒ–ï¼‰

```python
class CLIPWithCache:
    """
    å¸¦ç‰¹å¾ç¼“å­˜çš„ CLIPï¼ˆç”¨äºå¤§è§„æ¨¡æ£€ç´¢ï¼‰
    """
    def __init__(self, model, cache_dir='./cache'):
        self.model = model
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def encode_and_cache(self, images, cache_key):
        """ç¼–ç å¹¶ç¼“å­˜ç‰¹å¾"""
        cache_path = os.path.join(self.cache_dir, f"{cache_key}.pt")
        
        if os.path.exists(cache_path):
            return torch.load(cache_path)
        
        with torch.no_grad():
            features = self.model.encode_image(images)
            torch.save(features, cache_path)
        
        return features
```

### 8.6 æ‰¹é‡æ¨ç†ä¼˜åŒ–

```python
def batch_encode_images(model, images, batch_size=32):
    """
    æ‰¹é‡ç¼–ç å›¾åƒï¼ˆé¿å… OOMï¼‰
    """
    features_list = []
    for i in range(0, len(images), batch_size):
        batch = images[i:i+batch_size]
        with torch.no_grad():
            features = model.encode_image(batch)
            features_list.append(features.cpu())
    return torch.cat(features_list, dim=0)
```

### 8.7 è®­ç»ƒç›‘æ§ä¸è°ƒè¯•

```python
def monitor_training(model, dataloader, device):
    """
    ç›‘æ§è®­ç»ƒè¿‡ç¨‹çš„å…³é”®æŒ‡æ ‡
    """
    model.eval()
    total_loss = 0.0
    total_acc_img = 0.0
    total_acc_txt = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for images, texts in dataloader:
            images = images.to(device)
            texts = texts.to(device)
            
            loss, logits_img, logits_txt = model(images, texts)
            
            # è®¡ç®—å‡†ç¡®ç‡
            acc_img, acc_txt = model.loss_fn.get_accuracy(logits_img, logits_txt)
            
            total_loss += loss.item()
            total_acc_img += acc_img
            total_acc_txt += acc_txt
            num_batches += 1
    
    return {
        'loss': total_loss / num_batches,
        'acc_image_to_text': total_acc_img / num_batches,
        'acc_text_to_image': total_acc_txt / num_batches,
        'temperature': model.loss_fn.temperature
    }
```

### 8.8 å¸¸è§è®­ç»ƒé—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

**é—®é¢˜1ï¼šæŸå¤±ä¸ä¸‹é™**
- **åŸå› **ï¼šå­¦ä¹ ç‡è¿‡å¤§æˆ–è¿‡å°
- **è§£å†³**ï¼šè°ƒæ•´å­¦ä¹ ç‡ï¼Œä½¿ç”¨ warmup

**é—®é¢˜2ï¼šå‡†ç¡®ç‡ä¸æå‡**
- **åŸå› **ï¼šæ‰¹æ¬¡å¤ªå°ï¼Œè´Ÿæ ·æœ¬ä¸è¶³
- **è§£å†³**ï¼šå¢åŠ æ‰¹æ¬¡å¤§å°æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

**é—®é¢˜3ï¼šæ¸©åº¦å‚æ•°å‘æ•£**
- **åŸå› **ï¼šlogit_scale åˆå§‹åŒ–ä¸å½“
- **è§£å†³**ï¼šä½¿ç”¨ log(1/0.07) åˆå§‹åŒ–ï¼Œæ·»åŠ æ¢¯åº¦è£å‰ª

**é—®é¢˜4ï¼šå†…å­˜æº¢å‡ºï¼ˆOOMï¼‰**
- **åŸå› **ï¼šæ‰¹æ¬¡å¤ªå¤§æˆ–æ¨¡å‹å¤ªå¤§
- **è§£å†³**ï¼šå‡å°æ‰¹æ¬¡ï¼Œä½¿ç”¨æ··åˆç²¾åº¦ï¼Œæ¢¯åº¦æ£€æŸ¥ç‚¹

**é—®é¢˜5ï¼šè®­ç»ƒä¸ç¨³å®š**
- **åŸå› **ï¼šæ¢¯åº¦çˆ†ç‚¸æˆ–å­¦ä¹ ç‡è°ƒåº¦ä¸å½“
- **è§£å†³**ï¼šæ¢¯åº¦è£å‰ªï¼Œä½¿ç”¨ cosine decay

---

## ä¹ã€å®é™…åº”ç”¨æ¡ˆä¾‹

### 9.1 å›¾åƒæœç´¢ï¼ˆGoogle Photos é£æ ¼ï¼‰

```python
def image_search(query_text, image_database, model, top_k=10):
    """
    å›¾åƒæœç´¢ï¼šæ ¹æ®æ–‡æœ¬æŸ¥è¯¢æ‰¾åˆ°ç›¸ä¼¼å›¾åƒ
    """
    # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
    query_features = model.encode_text([query_text])
    
    # æ‰¹é‡ç¼–ç å›¾åƒåº“
    image_features = batch_encode_images(model, image_database)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarity = query_features @ image_features.t()
    
    # è¿”å› top-k
    top_scores, top_indices = torch.topk(similarity, k=top_k)
    
    return [(idx.item(), score.item()) for score, idx in zip(top_scores[0], top_indices[0])]
```

### 9.2 å†…å®¹å®¡æ ¸

```python
def content_moderation(image, model, prohibited_concepts):
    """
    å†…å®¹å®¡æ ¸ï¼šæ£€æµ‹å›¾åƒæ˜¯å¦åŒ…å«ç¦æ­¢æ¦‚å¿µ
    """
    # æ„å»ºæç¤º
    texts = [f"a photo of {concept}" for concept in prohibited_concepts]
    
    # ç¼–ç 
    image_features = model.encode_image(image)
    text_features = model.encode_text(texts)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarity = image_features @ text_features.t()
    
    # é˜ˆå€¼åˆ¤æ–­
    threshold = 0.3
    violations = [
        (concept, sim.item())
        for concept, sim in zip(prohibited_concepts, similarity[0])
        if sim.item() > threshold
    ]
    
    return violations
```

### 9.3 å›¾åƒæ ‡æ³¨

```python
def auto_image_caption(image, model, candidate_captions):
    """
    è‡ªåŠ¨å›¾åƒæ ‡æ³¨ï¼šä»å€™é€‰æè¿°ä¸­é€‰æ‹©æœ€åˆé€‚çš„
    """
    image_features = model.encode_image(image)
    text_features = model.encode_text(candidate_captions)
    
    similarity = image_features @ text_features.t()
    best_idx = torch.argmax(similarity, dim=1)
    
    return candidate_captions[best_idx.item()]
```

---

## åã€æ€§èƒ½åŸºå‡†ä¸å®éªŒæ•°æ®

### 10.1 ImageNet é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½

| æ¨¡å‹ | å‚æ•°é‡ | ImageNet Top-1 | ImageNet Top-5 |
|------|--------|---------------|---------------|
| CLIP ViT-B/32 | 151M | 63.2% | 85.1% |
| CLIP ViT-B/16 | 151M | 68.3% | 88.9% |
| CLIP ViT-L/14 | 428M | 75.5% | 92.1% |
| CLIP ViT-L/14@336px | 428M | **76.6%** | **92.5%** |

**å…³é”®è§‚å¯Ÿï¼š**
- æ›´å¤§æ¨¡å‹æ€§èƒ½æ›´å¥½ï¼ˆScaling Lawï¼‰
- æ›´é«˜åˆ†è¾¨ç‡ï¼ˆ336pxï¼‰æå‡æ€§èƒ½
- é›¶æ ·æœ¬æ€§èƒ½æ¥è¿‘æœ‰ç›‘ç£ ResNet-50ï¼ˆ76.0%ï¼‰

### 10.2 ä¸åŒæ•°æ®é›†çš„é›¶æ ·æœ¬æ€§èƒ½

| æ•°æ®é›† | CLIP ViT-L/14 | æœ‰ç›‘ç£ SOTA |
|--------|--------------|------------|
| ImageNet | 76.6% | 90.9% |
| CIFAR-10 | 95.2% | 99.5% |
| CIFAR-100 | 77.9% | 95.7% |
| STL-10 | 99.4% | 99.9% |
| Food-101 | 90.1% | 90.4% |

**åˆ†æï¼š**
- è‡ªç„¶å›¾åƒæ•°æ®é›†è¡¨ç°å¥½ï¼ˆæ¥è¿‘æœ‰ç›‘ç£ï¼‰
- ç»†ç²’åº¦æ•°æ®é›†è¡¨ç°è¾ƒå·®
- æ•°æ®åˆ†å¸ƒä¸é¢„è®­ç»ƒæ•°æ®ç›¸ä¼¼æ—¶æ€§èƒ½æ›´å¥½

### 10.3 æç¤ºå·¥ç¨‹çš„å½±å“

| æç¤ºç­–ç•¥ | ImageNet å‡†ç¡®ç‡ | æå‡ |
|---------|---------------|------|
| æ— æç¤ºï¼ˆç›´æ¥ç±»åˆ«åï¼‰ | 74.9% | - |
| å•ä¸€æ¨¡æ¿ "a photo of a {}" | 76.2% | +1.3% |
| 80 ä¸ªæ¨¡æ¿å¹³å‡ | **76.6%** | +1.7% |

**ç»“è®ºï¼š**
- æç¤ºå·¥ç¨‹æ˜¾è‘—æå‡æ€§èƒ½
- å¤šæ¨¡æ¿å¹³å‡æ›´é²æ£’
- æ¨¡æ¿è´¨é‡æ¯”æ•°é‡æ›´é‡è¦

### 10.4 è®¡ç®—æ•ˆç‡å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | æ¨ç†æ—¶é—´ï¼ˆmsï¼‰ | å†…å­˜ï¼ˆGBï¼‰ |
|------|--------|--------------|----------|
| CLIP ViT-B/32 | 151M | 12 | 0.6 |
| CLIP ViT-B/16 | 151M | 18 | 0.8 |
| CLIP ViT-L/14 | 428M | 45 | 1.8 |

**ä¼˜åŒ–å»ºè®®ï¼š**
- ä½¿ç”¨ ViT-B/32 å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦
- é‡åŒ–å¯å‡å°‘ 50% å†…å­˜
- ONNX/TensorRT å¯åŠ é€Ÿ 2-3 å€

---

## åä¸€ã€éƒ¨ç½²è€ƒè™‘

### 10.1 æ¨¡å‹é‡åŒ–

```python
import torch.quantization as quantization

def quantize_clip(model):
    """é‡åŒ– CLIP æ¨¡å‹ï¼ˆINT8ï¼‰"""
    model.eval()
    
    # å‡†å¤‡é‡åŒ–
    model.qconfig = quantization.get_default_qconfig('fbgemm')
    quantization.prepare(model, inplace=True)
    
    # æ ¡å‡†ï¼ˆä½¿ç”¨å°‘é‡æ•°æ®ï¼‰
    # calibration_data = ...
    # model(calibration_data)
    
    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    quantized_model = quantization.convert(model, inplace=False)
    
    return quantized_model
```

### 10.2 ONNX å¯¼å‡º

```python
def export_to_onnx(model, sample_image, sample_text, output_path):
    """å¯¼å‡º CLIP åˆ° ONNX"""
    model.eval()
    
    # å¯¼å‡ºå›¾åƒç¼–ç å™¨
    torch.onnx.export(
        model.image_encoder,
        sample_image,
        f"{output_path}_image.onnx",
        input_names=['image'],
        output_names=['image_features'],
        dynamic_axes={'image': {0: 'batch_size'}}
    )
    
    # å¯¼å‡ºæ–‡æœ¬ç¼–ç å™¨
    torch.onnx.export(
        model.text_encoder,
        sample_text,
        f"{output_path}_text.onnx",
        input_names=['text'],
        output_names=['text_features'],
        dynamic_axes={'text': {0: 'batch_size'}}
    )
```

### 10.3 TensorRT ä¼˜åŒ–

```python
# ä½¿ç”¨ TensorRT åŠ é€Ÿæ¨ç†
# éœ€è¦å…ˆè½¬æ¢ä¸º ONNXï¼Œç„¶åä½¿ç”¨ TensorRT
```

---

## åäºŒã€æ€»ç»“ä¸å…³é”®è¦ç‚¹

### 12.1 æ ¸å¿ƒè¦ç‚¹

1. **å¯¹ç§° InfoNCE æŸå¤±**ï¼šCLIP ä½¿ç”¨åŒå‘å¯¹æ¯”å­¦ä¹ ï¼Œç¡®ä¿å›¾åƒå’Œæ–‡æœ¬åŒå‘å¯¹é½
2. **å¯å­¦ä¹ æ¸©åº¦**ï¼šlogit_scale åœ¨ log ç©ºé—´åˆå§‹åŒ–ï¼Œè‡ªåŠ¨å­¦ä¹ æœ€ä¼˜æ¸©åº¦
3. **å¤§è§„æ¨¡é¢„è®­ç»ƒ**ï¼š4 äº¿å›¾æ–‡å¯¹ï¼Œå®ç°å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›
4. **ç®€å•æ¶æ„**ï¼šåŒå¡”ç»“æ„ï¼Œæ˜“äºæ‰©å±•å’Œåº”ç”¨
5. **æ¢¯åº¦åˆ†æ**ï¼šå¯¹ç§°æŸå¤±ç¡®ä¿åŒå‘æ¢¯åº¦ä¸€è‡´ï¼Œè®­ç»ƒæ›´ç¨³å®š
6. **äº’ä¿¡æ¯ä¸‹ç•Œ**ï¼šCLIP æŸå¤±æœ€å¤§åŒ–å›¾åƒå’Œæ–‡æœ¬çš„äº’ä¿¡æ¯

### 12.2 é¢è¯•å›ç­”æ¨¡æ¿

> CLIP ä½¿ç”¨å¯¹ç§° InfoNCE æŸå¤±å‡½æ•°ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€è¡¨ç¤ºç©ºé—´ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ä¸¤ä¸ªæ–¹å‘ï¼šå›¾åƒâ†’æ–‡æœ¬å’Œæ–‡æœ¬â†’å›¾åƒï¼Œå–å¹³å‡å€¼å¾—åˆ°å¯¹ç§°æŸå¤±ã€‚å…³é”®åˆ›æ–°æ˜¯ä½¿ç”¨å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°ï¼ˆlogit_scaleï¼‰ï¼Œåœ¨ log ç©ºé—´åˆå§‹åŒ–ç¡®ä¿æ¸©åº¦å§‹ç»ˆä¸ºæ­£ã€‚ä»æ•°å­¦è§’åº¦çœ‹ï¼ŒCLIP æŸå¤±æœ€å¤§åŒ–å›¾åƒå’Œæ–‡æœ¬çš„äº’ä¿¡æ¯ï¼Œæ˜¯å¯¹ç§° InfoNCE åœ¨å¤šæ¨¡æ€åœºæ™¯çš„åº”ç”¨ã€‚é€šè¿‡ 4 äº¿å›¾æ–‡å¯¹çš„å¤§è§„æ¨¡é¢„è®­ç»ƒï¼ŒCLIP å®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œå›¾æ–‡æ£€ç´¢èƒ½åŠ›ã€‚

### 12.3 å…³é”®é¢è¯•é—®é¢˜æ€»ç»“

1. **æŸå¤±å‡½æ•°**ï¼šå¯¹ç§° InfoNCEï¼ŒåŒå‘å¯¹æ¯”å­¦ä¹ 
2. **æ¸©åº¦å‚æ•°**ï¼šå¯å­¦ä¹ ï¼Œlog ç©ºé—´åˆå§‹åŒ–
3. **æ‰¹æ¬¡å¤§å°**ï¼š32Kï¼Œå¤§æ‰¹æ¬¡æå‡æ€§èƒ½
4. **é›¶æ ·æœ¬åˆ†ç±»**ï¼šæç¤ºå·¥ç¨‹ï¼Œå¤šæ¨¡æ¿å¹³å‡
5. **ä¸å…¶ä»–æ–¹æ³•å¯¹æ¯”**ï¼šCLIP vs ALIGN vs BLIP
6. **æ¢¯åº¦åˆ†æ**ï¼šå¯¹ç§°æ€§ä¿è¯åŒå‘å¯¹é½
7. **äº’ä¿¡æ¯å…³ç³»**ï¼šCLIP æŸå¤±æ˜¯äº’ä¿¡æ¯çš„ä¸‹ç•Œ

### 12.4 è¿›ä¸€æ­¥å­¦ä¹ 

- **è®ºæ–‡**ï¼š
  - CLIP: Learning Transferable Visual Models from Natural Language Supervision (ICML 2021)
  - ALIGN: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision (ICML 2021)
  - BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (CVPR 2022)
  
- **ä»£ç åº“**ï¼š
  - OpenAI CLIP: https://github.com/openai/CLIP
  - Hugging Face: https://huggingface.co/docs/transformers/model_doc/clip
  - OpenCLIP: https://github.com/mlfoundations/open_clip

- **ç›¸å…³æ–¹æ³•**ï¼š
  - ALIGN: æ›´å¤§è§„æ¨¡æ•°æ®
  - BLIP: æ”¯æŒç”Ÿæˆä»»åŠ¡
  - Flamingo: å°‘æ ·æœ¬å­¦ä¹ 
  - CoCa: ç»Ÿä¸€ç¼–ç å™¨-è§£ç å™¨æ¶æ„

---

**æœ€åæ›´æ–°ï¼š** 2024å¹´
